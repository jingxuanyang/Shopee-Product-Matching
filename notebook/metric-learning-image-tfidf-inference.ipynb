{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016151,
     "end_time": "2021-03-31T13:06:42.556493",
     "exception": false,
     "start_time": "2021-03-31T13:06:42.540342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# About this Notebook\n",
    "\n",
    "Hi all this is the inference notebook for the training notebook found [here](https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=57596864) and is a Pytorch Implementation of kernel given by @ragnar from [here](https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface)\n",
    "\n",
    "What we are using in inference :\n",
    "* Effnet-B3 Trained with arc-face and Cross Entropy loss for Images\n",
    "* TFiDF for texts\n",
    "\n",
    "I am able to achieve 0.712 lb using the training and this inference notebook without any changes on the baseline. I will be adding training and inference code for transformer model on texts as well\n",
    "\n",
    "This notebook runs without errors for all the efficientnet architectures\n",
    "\n",
    "I am quick saving notebook for now as I don't have GPU left , I will commit and get an lb score on this on the weekend ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:42.591024Z",
     "iopub.status.busy": "2021-03-31T13:06:42.590288Z",
     "iopub.status.idle": "2021-03-31T13:06:42.592955Z",
     "shell.execute_reply": "2021-03-31T13:06:42.593324Z"
    },
    "papermill": {
     "duration": 0.022259,
     "end_time": "2021-03-31T13:06:42.593562",
     "exception": false,
     "start_time": "2021-03-31T13:06:42.571303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:42.629127Z",
     "iopub.status.busy": "2021-03-31T13:06:42.628499Z",
     "iopub.status.idle": "2021-03-31T13:06:52.231388Z",
     "shell.execute_reply": "2021-03-31T13:06:52.230243Z"
    },
    "papermill": {
     "duration": 9.622998,
     "end_time": "2021-03-31T13:06:52.231524",
     "exception": false,
     "start_time": "2021-03-31T13:06:42.608526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visuals and CV2\n",
    "import cv2\n",
    "\n",
    "# albumentations for augs\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "# import cudf\n",
    "# import cuml\n",
    "# import cupy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.271981Z",
     "iopub.status.busy": "2021-03-31T13:06:52.271436Z",
     "iopub.status.idle": "2021-03-31T13:06:52.284118Z",
     "shell.execute_reply": "2021-03-31T13:06:52.283534Z"
    },
    "papermill": {
     "duration": 0.037286,
     "end_time": "2021-03-31T13:06:52.284267",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.246981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "this submission notebook will compute CV score, but commit notebook will not\n"
     ]
    }
   ],
   "source": [
    "DIM = (512,512)\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SEED = 2020\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "CLASSES = 11014\n",
    "\n",
    "################################################  ADJUSTING FOR CV OR SUBMIT ##############################################\n",
    "\n",
    "CHECK_SUB = False\n",
    "GET_CV = True\n",
    "\n",
    "test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "if len(test)>3: GET_CV = False\n",
    "else: print('this submission notebook will compute CV score, but commit notebook will not')\n",
    "\n",
    "\n",
    "################################################# MODEL ####################################################################\n",
    "\n",
    "model_name = 'efficientnet_b3' #efficientnet_b0-b7\n",
    "\n",
    "################################################ MODEL PATH ###############################################################\n",
    "\n",
    "IMG_MODEL_PATH = '../input/pytorch-metric-learning-pipeline-only-images/model_efficientnet_b3_IMG_SIZE_512_arcface.bin'\n",
    "\n",
    "################################################ Metric Loss and its params #######################################################\n",
    "loss_module = 'arcface' #'cosface' #'adacos'\n",
    "s = 30.0\n",
    "m = 0.5 \n",
    "ls_eps = 0.0\n",
    "easy_margin = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015082,
     "end_time": "2021-03-31T13:06:52.315307",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.300225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.353369Z",
     "iopub.status.busy": "2021-03-31T13:06:52.352668Z",
     "iopub.status.idle": "2021-03-31T13:06:52.355607Z",
     "shell.execute_reply": "2021-03-31T13:06:52.355176Z"
    },
    "papermill": {
     "duration": 0.024927,
     "end_time": "2021-03-31T13:06:52.355714",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.330787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        # df_cu = cudf.DataFrame(df)\n",
    "        df_cu = df\n",
    "        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n",
    "    else:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        # df_cu = cudf.DataFrame(df)\n",
    "        df_cu = df\n",
    "        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "        \n",
    "    return df, df_cu, image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015094,
     "end_time": "2021-03-31T13:06:52.387016",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.371922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.422179Z",
     "iopub.status.busy": "2021-03-31T13:06:52.421672Z",
     "iopub.status.idle": "2021-03-31T13:06:52.427772Z",
     "shell.execute_reply": "2021-03-31T13:06:52.427196Z"
    },
    "papermill": {
     "duration": 0.025666,
     "end_time": "2021-03-31T13:06:52.427883",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.402217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.465093Z",
     "iopub.status.busy": "2021-03-31T13:06:52.464455Z",
     "iopub.status.idle": "2021-03-31T13:06:52.467343Z",
     "shell.execute_reply": "2021-03-31T13:06:52.466944Z"
    },
    "papermill": {
     "duration": 0.023957,
     "end_time": "2021-03-31T13:06:52.467451",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.443494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.503979Z",
     "iopub.status.busy": "2021-03-31T13:06:52.502271Z",
     "iopub.status.idle": "2021-03-31T13:06:52.504618Z",
     "shell.execute_reply": "2021-03-31T13:06:52.505063Z"
    },
    "papermill": {
     "duration": 0.022172,
     "end_time": "2021-03-31T13:06:52.505185",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.483013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n",
    "    return ' '.join( np.unique(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.551353Z",
     "iopub.status.busy": "2021-03-31T13:06:52.550655Z",
     "iopub.status.idle": "2021-03-31T13:06:52.553727Z",
     "shell.execute_reply": "2021-03-31T13:06:52.554148Z"
    },
    "papermill": {
     "duration": 0.032724,
     "end_time": "2021-03-31T13:06:52.554340",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.521616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_neighbors(df, embeddings, KNN = 50, image = True):\n",
    "    '''\n",
    "    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n",
    "    '''\n",
    "\n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "    if GET_CV:\n",
    "        if image:\n",
    "            thresholds = list(np.arange(2,4,0.1))\n",
    "        else:\n",
    "            thresholds = list(np.arange(0.1, 1, 0.1))\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            predictions = []\n",
    "            for k in range(embeddings.shape[0]):\n",
    "                idx = np.where(distances[k,] < threshold)[0]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "                predictions.append(posting_ids)\n",
    "            df['pred_matches'] = predictions\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "        \n",
    "        # Use threshold\n",
    "        predictions = []\n",
    "        for k in range(embeddings.shape[0]):\n",
    "            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < 2.7)[0]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 0.60)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "    \n",
    "    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "    else:\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < 2.7)[0]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 0.60)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017567,
     "end_time": "2021-03-31T13:06:52.588011",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.570444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.626090Z",
     "iopub.status.busy": "2021-03-31T13:06:52.624470Z",
     "iopub.status.idle": "2021-03-31T13:06:52.626668Z",
     "shell.execute_reply": "2021-03-31T13:06:52.627089Z"
    },
    "papermill": {
     "duration": 0.023028,
     "end_time": "2021-03-31T13:06:52.627212",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.604184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "\n",
    "    return albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n",
    "            albumentations.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.666435Z",
     "iopub.status.busy": "2021-03-31T13:06:52.664758Z",
     "iopub.status.idle": "2021-03-31T13:06:52.667068Z",
     "shell.execute_reply": "2021-03-31T13:06:52.667466Z"
    },
    "papermill": {
     "duration": 0.024635,
     "end_time": "2021-03-31T13:06:52.667583",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.642948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "        \n",
    "        \n",
    "        return image,torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.714581Z",
     "iopub.status.busy": "2021-03-31T13:06:52.712952Z",
     "iopub.status.idle": "2021-03-31T13:06:52.715293Z",
     "shell.execute_reply": "2021-03-31T13:06:52.715688Z"
    },
    "papermill": {
     "duration": 0.032024,
     "end_time": "2021-03-31T13:06:52.715829",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.683805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 model_name='efficientnet_b0',\n",
    "                 use_fc=False,\n",
    "                 fc_dim=512,\n",
    "                 dropout=0.0,\n",
    "                 loss_module='softmax',\n",
    "                 s=30.0,\n",
    "                 margin=0.50,\n",
    "                 ls_eps=0.0,\n",
    "                 theta_zero=0.785,\n",
    "                 pretrained=False):\n",
    "        \"\"\"\n",
    "        :param n_classes:\n",
    "        :param model_name: name of model from pretrainedmodels\n",
    "            e.g. resnet50, resnext101_32x4d, pnasnet5large\n",
    "        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n",
    "        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n",
    "        \"\"\"\n",
    "        super(ShopeeNet, self).__init__()\n",
    "        print('Model building for {} backbone'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        final_in_features = self.backbone.classifier.in_features\n",
    "        \n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.use_fc = use_fc\n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "        self.loss_module = loss_module\n",
    "        if loss_module == 'arcface':\n",
    "            self.final = ArcMarginProduct(final_in_features, n_classes,\n",
    "                                          s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n",
    "        elif loss_module == 'cosface':\n",
    "            self.final = AddMarginProduct(final_in_features, n_classes, s=s, m=margin)\n",
    "        elif loss_module == 'adacos':\n",
    "            self.final = AdaCos(final_in_features, n_classes, m=margin, theta_zero=theta_zero)\n",
    "        else:\n",
    "            self.final = nn.Linear(final_in_features, n_classes)\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        feature = self.extract_feat(x)\n",
    "        if self.loss_module in ('arcface', 'cosface', 'adacos'):\n",
    "            logits = self.final(feature, label)\n",
    "        else:\n",
    "            logits = self.final(feature)\n",
    "        return feature,logits\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.758426Z",
     "iopub.status.busy": "2021-03-31T13:06:52.757796Z",
     "iopub.status.idle": "2021-03-31T13:06:52.761172Z",
     "shell.execute_reply": "2021-03-31T13:06:52.760778Z"
    },
    "papermill": {
     "duration": 0.029574,
     "end_time": "2021-03-31T13:06:52.761279",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.731705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaCos(nn.Module):\n",
    "    def __init__(self, in_features, out_features, m=0.50, ls_eps=0, theta_zero=math.pi/4):\n",
    "        super(AdaCos, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.theta_zero = theta_zero\n",
    "        self.s = math.log(out_features - 1) / math.cos(theta_zero)\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # normalize features\n",
    "        x = F.normalize(input)\n",
    "        # normalize weights\n",
    "        W = F.normalize(self.weight)\n",
    "        # dot product\n",
    "        logits = F.linear(x, W)\n",
    "        # add margin\n",
    "        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        output = logits * (1 - one_hot) + target_logits * one_hot\n",
    "        # feature re-scale\n",
    "        with torch.no_grad():\n",
    "            B_avg = torch.where(one_hot < 1, torch.exp(self.s * logits), torch.zeros_like(logits))\n",
    "            B_avg = torch.sum(B_avg) / input.size(0)\n",
    "            theta_med = torch.median(theta)\n",
    "            self.s = torch.log(B_avg) / torch.cos(torch.min(self.theta_zero * torch.ones_like(theta_med), theta_med))\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.804442Z",
     "iopub.status.busy": "2021-03-31T13:06:52.803856Z",
     "iopub.status.idle": "2021-03-31T13:06:52.806915Z",
     "shell.execute_reply": "2021-03-31T13:06:52.806391Z"
    },
    "papermill": {
     "duration": 0.029691,
     "end_time": "2021-03-31T13:06:52.807021",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.777330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.846968Z",
     "iopub.status.busy": "2021-03-31T13:06:52.846336Z",
     "iopub.status.idle": "2021-03-31T13:06:52.848827Z",
     "shell.execute_reply": "2021-03-31T13:06:52.849191Z"
    },
    "papermill": {
     "duration": 0.026055,
     "end_time": "2021-03-31T13:06:52.849319",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.823264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin cosine distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta) - m\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n",
    "        super(AddMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        phi = cosine - self.m\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.888983Z",
     "iopub.status.busy": "2021-03-31T13:06:52.888345Z",
     "iopub.status.idle": "2021-03-31T13:06:52.891147Z",
     "shell.execute_reply": "2021-03-31T13:06:52.890737Z"
    },
    "papermill": {
     "duration": 0.025483,
     "end_time": "2021-03-31T13:06:52.891251",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.865768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_paths):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeNet(n_classes=CLASSES,model_name=model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    model.load_state_dict(torch.load(IMG_MODEL_PATH),strict=False)\n",
    "    model = model.to(device)\n",
    "\n",
    "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat, _ = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016122,
     "end_time": "2021-03-31T13:06:52.923875",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.907753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using Texts with TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:52.962351Z",
     "iopub.status.busy": "2021-03-31T13:06:52.961581Z",
     "iopub.status.idle": "2021-03-31T13:06:52.964614Z",
     "shell.execute_reply": "2021-03-31T13:06:52.964084Z"
    },
    "papermill": {
     "duration": 0.024493,
     "end_time": "2021-03-31T13:06:52.964757",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.940264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embeddings(df_cu, max_features = 15000, n_components = 5000):\n",
    "    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n",
    "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "    pca = PCA(n_components = n_components)\n",
    "    text_embeddings = pca.fit_transform(text_embeddings).get()\n",
    "    print(f'Our title text embedding shape is {text_embeddings.shape}')\n",
    "    del model, pca\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016336,
     "end_time": "2021-03-31T13:06:52.997664",
     "exception": false,
     "start_time": "2021-03-31T13:06:52.981328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Calculating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:06:53.037035Z",
     "iopub.status.busy": "2021-03-31T13:06:53.036495Z",
     "iopub.status.idle": "2021-03-31T13:07:03.229435Z",
     "shell.execute_reply": "2021-03-31T13:07:03.229014Z"
    },
    "papermill": {
     "duration": 10.215199,
     "end_time": "2021-03-31T13:07:03.229561",
     "exception": false,
     "start_time": "2021-03-31T13:06:53.014362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                             matches  \n",
       "0   train_129225211 train_2278313361  \n",
       "1  train_3386243561 train_3423213080  \n",
       "2  train_2288590299 train_3803689425  \n",
       "3  train_2406599165 train_3342059966  \n",
       "4   train_3369186413 train_921438619  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>train_3386243561 train_3423213080</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>train_2406599165 train_3342059966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>train_3369186413 train_921438619</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "df,df_cu,image_paths = read_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:07:03.270115Z",
     "iopub.status.busy": "2021-03-31T13:07:03.269537Z",
     "iopub.status.idle": "2021-03-31T13:17:14.261791Z",
     "shell.execute_reply": "2021-03-31T13:17:14.262414Z"
    },
    "papermill": {
     "duration": 611.015552,
     "end_time": "2021-03-31T13:17:14.262585",
     "exception": false,
     "start_time": "2021-03-31T13:07:03.247033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building for efficientnet_b3 backbone\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5317a6fdf6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-a4501a54e733>\u001b[0m in \u001b[0;36mget_image_embeddings\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caslx/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "image_embeddings = get_image_embeddings(image_paths.values)\n",
    "text_embeddings = get_text_embeddings(df_cu, max_features = 15000, n_components = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:17:15.709160Z",
     "iopub.status.busy": "2021-03-31T13:17:15.708181Z",
     "iopub.status.idle": "2021-03-31T13:18:17.460142Z",
     "shell.execute_reply": "2021-03-31T13:18:17.459582Z"
    },
    "papermill": {
     "duration": 62.474134,
     "end_time": "2021-03-31T13:18:17.460287",
     "exception": false,
     "start_time": "2021-03-31T13:17:14.986153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1 score for threshold 2.0 is 0.6746142458627089\n",
      "Our f1 score for threshold 2.1 is 0.6824880908437616\n",
      "Our f1 score for threshold 2.2 is 0.6907857766613116\n",
      "Our f1 score for threshold 2.3000000000000003 is 0.6979360720386149\n",
      "Our f1 score for threshold 2.4000000000000004 is 0.7046210864396953\n",
      "Our f1 score for threshold 2.5000000000000004 is 0.7122625175497008\n",
      "Our f1 score for threshold 2.6000000000000005 is 0.7192237734761231\n",
      "Our f1 score for threshold 2.7000000000000006 is 0.7256486764786477\n",
      "Our f1 score for threshold 2.8000000000000007 is 0.7308335598535366\n",
      "Our f1 score for threshold 2.900000000000001 is 0.7342980476098994\n",
      "Our f1 score for threshold 3.000000000000001 is 0.7336691474781065\n",
      "Our f1 score for threshold 3.100000000000001 is 0.7260412733085204\n",
      "Our f1 score for threshold 3.200000000000001 is 0.712987042135018\n",
      "Our f1 score for threshold 3.300000000000001 is 0.6897795932656223\n",
      "Our f1 score for threshold 3.4000000000000012 is 0.6544315932663987\n",
      "Our f1 score for threshold 3.5000000000000013 is 0.6072139746249673\n",
      "Our f1 score for threshold 3.6000000000000014 is 0.5509345686413814\n",
      "Our f1 score for threshold 3.7000000000000015 is 0.4898803983071468\n",
      "Our f1 score for threshold 3.8000000000000016 is 0.4277737035585224\n",
      "Our f1 score for threshold 3.9000000000000017 is 0.3694927005158118\n",
      "Our best score is 0.7342980476098994 and has a threshold 2.900000000000001\n"
     ]
    }
   ],
   "source": [
    "# Get neighbors for image_embeddings\n",
    "df,image_predictions = get_neighbors(df, image_embeddings, KNN = 50, image = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:18:19.186671Z",
     "iopub.status.busy": "2021-03-31T13:18:19.186133Z",
     "iopub.status.idle": "2021-03-31T13:18:19.191346Z",
     "shell.execute_reply": "2021-03-31T13:18:19.191767Z"
    },
    "papermill": {
     "duration": 1.010539,
     "end_time": "2021-03-31T13:18:19.191907",
     "exception": false,
     "start_time": "2021-03-31T13:18:18.181368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>pred_matches</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>train_129225211 train_2113554964 train_1100764...</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "      <td>train_3386243561 train_3423213080 train_212059...</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>train_2406599165 train_2088327894 train_256062...</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "      <td>train_3369186413 train_921438619 train_1054326...</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                             matches  \\\n",
       "0   train_129225211 train_2278313361   \n",
       "1  train_3386243561 train_3423213080   \n",
       "2  train_2288590299 train_3803689425   \n",
       "3  train_2406599165 train_3342059966   \n",
       "4   train_3369186413 train_921438619   \n",
       "\n",
       "                                        pred_matches        f1  \n",
       "0  train_129225211 train_2113554964 train_1100764...  0.068966  \n",
       "1  train_3386243561 train_3423213080 train_212059...  0.266667  \n",
       "2                                   train_2288590299  0.666667  \n",
       "3  train_2406599165 train_2088327894 train_256062...  0.064516  \n",
       "4  train_3369186413 train_921438619 train_1054326...  0.076923  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:18:20.718989Z",
     "iopub.status.busy": "2021-03-31T13:18:20.718143Z",
     "iopub.status.idle": "2021-03-31T13:18:50.515756Z",
     "shell.execute_reply": "2021-03-31T13:18:50.515197Z"
    },
    "papermill": {
     "duration": 30.545216,
     "end_time": "2021-03-31T13:18:50.515914",
     "exception": false,
     "start_time": "2021-03-31T13:18:19.970698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1 score for threshold 0.1 is 0.4983596327413812\n",
      "Our f1 score for threshold 0.2 is 0.502388009452807\n",
      "Our f1 score for threshold 0.30000000000000004 is 0.5145635969431641\n",
      "Our f1 score for threshold 0.4 is 0.537488480632458\n",
      "Our f1 score for threshold 0.5 is 0.5686432647436647\n",
      "Our f1 score for threshold 0.6 is 0.6019218159502875\n",
      "Our f1 score for threshold 0.7000000000000001 is 0.6316518501170751\n",
      "Our f1 score for threshold 0.8 is 0.6349771385455252\n",
      "Our f1 score for threshold 0.9 is 0.5375289820838848\n",
      "Our best score is 0.6349771385455252 and has a threshold 0.8\n"
     ]
    }
   ],
   "source": [
    "# Get neighbors for text_embeddings\n",
    "df, text_predictions = get_neighbors(df, text_embeddings, KNN = 50, image = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:18:52.223428Z",
     "iopub.status.busy": "2021-03-31T13:18:52.222546Z",
     "iopub.status.idle": "2021-03-31T13:18:52.227064Z",
     "shell.execute_reply": "2021-03-31T13:18:52.227496Z"
    },
    "papermill": {
     "duration": 0.992157,
     "end_time": "2021-03-31T13:18:52.227638",
     "exception": false,
     "start_time": "2021-03-31T13:18:51.235481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>pred_matches</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>train_129225211 train_2278313361 train_5660825...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "      <td>train_3386243561 train_1831941588 train_380550...</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>train_2406599165 train_1744956981 train_357671...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "      <td>train_3369186413 train_921438619 train_2772640...</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                             matches  \\\n",
       "0   train_129225211 train_2278313361   \n",
       "1  train_3386243561 train_3423213080   \n",
       "2  train_2288590299 train_3803689425   \n",
       "3  train_2406599165 train_3342059966   \n",
       "4   train_3369186413 train_921438619   \n",
       "\n",
       "                                        pred_matches        f1  \n",
       "0  train_129225211 train_2278313361 train_5660825...  0.333333  \n",
       "1  train_3386243561 train_1831941588 train_380550...  0.307692  \n",
       "2                  train_2288590299 train_3803689425  1.000000  \n",
       "3  train_2406599165 train_1744956981 train_357671...  0.200000  \n",
       "4  train_3369186413 train_921438619 train_2772640...  0.285714  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.72915,
     "end_time": "2021-03-31T13:18:53.676959",
     "exception": false,
     "start_time": "2021-03-31T13:18:52.947809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T13:18:55.123766Z",
     "iopub.status.busy": "2021-03-31T13:18:55.123193Z",
     "iopub.status.idle": "2021-03-31T13:18:56.838931Z",
     "shell.execute_reply": "2021-03-31T13:18:56.838248Z"
    },
    "papermill": {
     "duration": 2.444034,
     "end_time": "2021-03-31T13:18:56.839076",
     "exception": false,
     "start_time": "2021-03-31T13:18:54.395042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final f1 cv score is 0.7681670468388513\n"
     ]
    }
   ],
   "source": [
    "if GET_CV:\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "    score = df['f1'].mean()\n",
    "    print(f'Our final f1 cv score is {score}')\n",
    "    df['matches'] = df['pred_matches']\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.722385,
     "end_time": "2021-03-31T13:18:58.302402",
     "exception": false,
     "start_time": "2021-03-31T13:18:57.580017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd00b64f3f517ef2f38123c9b9d844dc7ba7aeffcc4559b7061ceea5f8a66fe5b86",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 744.449172,
   "end_time": "2021-03-31T13:19:01.374712",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-31T13:06:36.925540",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}