{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3.8.3 64-bit ('base': conda)","name":"python383jvsc74a57bd00b64f3f517ef2f38123c9b9d844dc7ba7aeffcc4559b7061ceea5f8a66fe5b86"},"language_info":{"name":"python","version":"3.8.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# About this kernel \n","\n","+ densenet121\n","+ CurricularFace\n","+ Mish() activation\n","+ Ranger (RAdam + Lookahead) optimizer\n","+ margin = 0.6"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{}},{"cell_type":"code","source":["import sys\n","\n","sys.path.append('../input/shopee-competition-utils')\n","sys.path.insert(0,'../input/pytorch-image-models')"],"metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","\n","import torch \n","from torch import nn \n","from torch.utils.data import Dataset, DataLoader \n","\n","import albumentations\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","from custom_scheduler import ShopeeScheduler\n","from custom_activation import replace_activations, Mish\n","from custom_optimizer import Ranger\n","\n","import math \n","import cv2\n","import timm \n","import os\n","import random\n","import gc\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import GroupKFold\n","from sklearn.neighbors import NearestNeighbors\n","from tqdm.notebook import tqdm "],"metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## Config"],"metadata":{}},{"cell_type":"code","source":["class CFG: \n","    \n","    DATA_DIR = '../input/shopee-product-matching/train_images'\n","    TRAIN_CSV = '../input/shopee-product-matching/train.csv'\n","\n","    # data augmentation\n","    IMG_SIZE = 512\n","    MEAN = [0.485, 0.456, 0.406]\n","    STD = [0.229, 0.224, 0.225]\n","\n","    SEED = 2021\n","\n","    # data split\n","    N_SPLITS = 5\n","    TEST_FOLD = 0\n","    VALID_FOLD = 1\n","\n","    EPOCHS = 8\n","    BATCH_SIZE = 8\n","\n","    NUM_WORKERS = 4\n","    DEVICE = 'cuda:1'\n","\n","    CLASSES = 6609 \n","    SCALE = 30\n","    MARGIN = 0.6\n","\n","    MODEL_NAME = 'densenet121'\n","    MODEL_PATH = f'{MODEL_NAME}_curricular_face_epoch_{EPOCHS}_bs_{BATCH_SIZE}_margin_{MARGIN}.pt'\n","    FC_DIM = 512\n","    SCHEDULER_PARAMS = {\n","            \"lr_start\": 1e-5,\n","            \"lr_max\": 1e-5 * 32,\n","            \"lr_min\": 1e-6,\n","            \"lr_ramp_ep\": 5,\n","            \"lr_sus_ep\": 0,\n","            \"lr_decay\": 0.8,\n","        }"],"metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Augmentations"],"metadata":{}},{"cell_type":"code","source":["def get_train_transforms():\n","    return albumentations.Compose(\n","        [   \n","            albumentations.Resize(CFG.IMG_SIZE,CFG.IMG_SIZE,always_apply=True),\n","            albumentations.HorizontalFlip(p=0.5),\n","            albumentations.VerticalFlip(p=0.5),\n","            albumentations.Rotate(limit=120, p=0.8),\n","            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n","            albumentations.Normalize(mean=CFG.MEAN, std=CFG.STD),\n","            ToTensorV2(p=1.0),\n","        ]\n","    )\n","\n","def get_valid_transforms():\n","\n","    return albumentations.Compose(\n","        [\n","            albumentations.Resize(CFG.IMG_SIZE,CFG.IMG_SIZE,always_apply=True),\n","            albumentations.Normalize(mean=CFG.MEAN, std=CFG.STD),\n","            ToTensorV2(p=1.0)\n","        ]\n","    )\n","\n","def get_test_transforms():\n","\n","    return albumentations.Compose(\n","        [\n","            albumentations.Resize(CFG.IMG_SIZE,CFG.IMG_SIZE,always_apply=True),\n","            albumentations.Normalize(mean=CFG.MEAN, std=CFG.STD),\n","            ToTensorV2(p=1.0)\n","        ]\n","    )"],"metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"source":["## Reproducibility"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True # set True to be faster\n","\n","seed_everything(CFG.SEED)"]},{"cell_type":"markdown","source":["## Dataset "],"metadata":{}},{"cell_type":"code","source":["class ShopeeDataset(torch.utils.data.Dataset):\n","    \"\"\"for training\n","    \"\"\"\n","    def __init__(self,df, transform = None):\n","        self.df = df \n","        self.root_dir = CFG.DATA_DIR\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self,idx):\n","\n","        row = self.df.iloc[idx]\n","\n","        img_path = os.path.join(self.root_dir,row.image)\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        label = row.label_group\n","\n","        if self.transform:\n","            augmented = self.transform(image=image)\n","            image = augmented['image']\n","\n","        return {\n","            'image' : image,\n","            'label' : torch.tensor(label).long()\n","        }"],"metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["class ShopeeImageDataset(torch.utils.data.Dataset):\n","    \"\"\"for validating and test\n","    \"\"\"\n","    def __init__(self,df, transform = None):\n","        self.df = df \n","        self.root_dir = CFG.DATA_DIR\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self,idx):\n","\n","        row = self.df.iloc[idx]\n","\n","        img_path = os.path.join(self.root_dir,row.image)\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        label = row.label_group\n","\n","        if self.transform:\n","            augmented = self.transform(image=image)\n","            image = augmented['image']     \n","                \n","        return image,torch.tensor(1)"]},{"cell_type":"markdown","source":["## Curricular Face + NFNet-L0"],"metadata":{}},{"cell_type":"code","source":["'''\n","credit : https://github.com/HuangYG123/CurricularFace/blob/8b2f47318117995aa05490c05b455b113489917e/head/metrics.py#L70\n","'''\n","\n","def l2_norm(input, axis = 1):\n","    norm = torch.norm(input, 2, axis, True)\n","    output = torch.div(input, norm)\n","\n","    return output\n","\n","class CurricularFace(nn.Module):\n","    def __init__(self, in_features, out_features, s = 30, m = 0.50):\n","        super(CurricularFace, self).__init__()\n","\n","        print('Using Curricular Face')\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.m = m\n","        self.s = s\n","        self.cos_m = math.cos(m)\n","        self.sin_m = math.sin(m)\n","        self.threshold = math.cos(math.pi - m)\n","        self.mm = math.sin(math.pi - m) * m\n","        self.kernel = nn.Parameter(torch.Tensor(in_features, out_features))\n","        self.register_buffer('t', torch.zeros(1))\n","        nn.init.normal_(self.kernel, std=0.01)\n","\n","    def forward(self, embbedings, label):\n","        embbedings = l2_norm(embbedings, axis = 1)\n","        kernel_norm = l2_norm(self.kernel, axis = 0)\n","        cos_theta = torch.mm(embbedings, kernel_norm)\n","        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n","        with torch.no_grad():\n","            origin_cos = cos_theta.clone()\n","        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n","\n","        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n","        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n","        mask = cos_theta > cos_theta_m\n","        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)\n","\n","        hard_example = cos_theta[mask]\n","        with torch.no_grad():\n","            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n","        cos_theta[mask] = hard_example * (self.t + hard_example)\n","        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n","        output = cos_theta * self.s\n","        return output, nn.CrossEntropyLoss()(output,label)"],"metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["class ShopeeModel(nn.Module):\n","\n","    def __init__(\n","        self,\n","        n_classes = CFG.CLASSES,\n","        model_name = CFG.MODEL_NAME,\n","        fc_dim = CFG.FC_DIM,\n","        margin = CFG.MARGIN,\n","        scale = CFG.SCALE,\n","        use_fc = True,\n","        pretrained = True):\n","\n","\n","        super(ShopeeModel,self).__init__()\n","        print('Building Model Backbone for {} model'.format(model_name))\n","\n","        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n","\n","        if 'efficientnet' in model_name:\n","            final_in_features = self.backbone.classifier.in_features\n","            self.backbone.classifier = nn.Identity()\n","            self.backbone.global_pool = nn.Identity()\n","        \n","        elif 'resnet' in model_name:\n","            final_in_features = self.backbone.fc.in_features\n","            self.backbone.fc = nn.Identity()\n","            self.backbone.global_pool = nn.Identity()\n","\n","        elif 'resnext' in model_name:\n","            final_in_features = self.backbone.fc.in_features\n","            self.backbone.fc = nn.Identity()\n","            self.backbone.global_pool = nn.Identity()\n","\n","        elif 'densenet' in model_name:\n","            final_in_features = self.backbone.classifier.in_features\n","            self.backbone.classifier = nn.Identity()\n","            self.backbone.global_pool = nn.Identity()\n","\n","        elif 'nfnet' in model_name:\n","            final_in_features = self.backbone.head.fc.in_features\n","            self.backbone.head.fc = nn.Identity()\n","            self.backbone.head.global_pool = nn.Identity()\n","\n","        self.pooling =  nn.AdaptiveAvgPool2d(1)\n","\n","        self.use_fc = use_fc\n","\n","        if use_fc:\n","            self.dropout = nn.Dropout(p=0.0)\n","            self.fc = nn.Linear(final_in_features, fc_dim)\n","            self.bn = nn.BatchNorm1d(fc_dim)\n","            self._init_params()\n","            final_in_features = fc_dim\n","\n","        self.final = CurricularFace(final_in_features, \n","                                           n_classes, \n","                                           s=scale, \n","                                           m=margin)\n","\n","    def _init_params(self):\n","        nn.init.xavier_normal_(self.fc.weight)\n","        nn.init.constant_(self.fc.bias, 0)\n","        nn.init.constant_(self.bn.weight, 1)\n","        nn.init.constant_(self.bn.bias, 0)\n","\n","    def forward(self, image, label):\n","        feature = self.extract_feat(image)\n","        logits = self.final(feature,label)\n","        return logits\n","\n","    def extract_feat(self, x):\n","        batch_size = x.shape[0]\n","        x = self.backbone(x)\n","        x = self.pooling(x).view(batch_size, -1)\n","\n","        if self.use_fc:\n","            x = self.dropout(x)\n","            x = self.fc(x)\n","            x = self.bn(x)\n","        return x\n"],"metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## Engine"],"metadata":{}},{"cell_type":"code","source":["def train_fn(model, data_loader, optimizer, scheduler, i):\n","    model.train()\n","    fin_loss = 0.0\n","    tk = tqdm(data_loader, desc = \"Epoch\" + \" [TRAIN] \" + str(i+1))\n","\n","    for t,data in enumerate(tk):\n","        for k,v in data.items():\n","            data[k] = v.to(CFG.DEVICE)\n","        optimizer.zero_grad()\n","        _, loss = model(**data)\n","        loss.backward()\n","        optimizer.step() \n","        fin_loss += loss.item() \n","\n","        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n","\n","    scheduler.step()\n","\n","    return fin_loss / len(data_loader)\n","\n","def eval_fn(model, data_loader, i):\n","    model.eval()\n","    fin_loss = 0.0\n","    tk = tqdm(data_loader, desc = \"Epoch\" + \" [VALID] \" + str(i+1))\n","\n","    with torch.no_grad():\n","        for t,data in enumerate(tk):\n","            for k,v in data.items():\n","                data[k] = v.to(CFG.DEVICE)\n","            _, loss = model(**data)\n","            fin_loss += loss.item() \n","\n","            tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1))})\n","        return fin_loss / len(data_loader)"],"metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def read_dataset():\n","    df = pd.read_csv(CFG.TRAIN_CSV)\n","    df['matches'] = df.label_group.map(df.groupby('label_group').posting_id.agg('unique').to_dict())\n","    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n","\n","    gkf = GroupKFold(n_splits=CFG.N_SPLITS)\n","    df['fold'] = -1\n","    for i, (train_idx, valid_idx) in enumerate(gkf.split(X=df, groups=df['label_group'])):\n","        df.loc[valid_idx, 'fold'] = i\n","\n","    labelencoder= LabelEncoder()\n","    df['label_group'] = labelencoder.fit_transform(df['label_group'])\n","\n","    train_df = df[df['fold']!=CFG.TEST_FOLD].reset_index(drop=True)\n","    train_df = train_df[train_df['fold']!=CFG.VALID_FOLD].reset_index(drop=True)\n","    valid_df = df[df['fold']==CFG.VALID_FOLD].reset_index(drop=True)\n","    test_df = df[df['fold']==CFG.TEST_FOLD].reset_index(drop=True)\n","\n","    train_df['label_group'] = labelencoder.fit_transform(train_df['label_group'])\n","\n","    return train_df, valid_df, test_df"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def precision_score(y_true, y_pred):\n","    y_true = y_true.apply(lambda x: set(x.split()))\n","    y_pred = y_pred.apply(lambda x: set(x.split()))\n","    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n","    len_y_pred = y_pred.apply(lambda x: len(x)).values\n","    precision = intersection / len_y_pred\n","    return precision\n","\n","def recall_score(y_true, y_pred):\n","    y_true = y_true.apply(lambda x: set(x.split()))\n","    y_pred = y_pred.apply(lambda x: set(x.split()))\n","    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n","    len_y_true = y_true.apply(lambda x: len(x)).values\n","    recall = intersection / len_y_true\n","    return recall\n","\n","def f1_score(y_true, y_pred):\n","    y_true = y_true.apply(lambda x: set(x.split()))\n","    y_pred = y_pred.apply(lambda x: set(x.split()))\n","    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n","    len_y_pred = y_pred.apply(lambda x: len(x)).values\n","    len_y_true = y_true.apply(lambda x: len(x)).values\n","    f1 = 2 * intersection / (len_y_pred + len_y_true)\n","    return f1"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def get_valid_embeddings(df, model):\n","\n","    model.eval()\n","\n","    image_dataset = ShopeeImageDataset(df,transform=get_valid_transforms())\n","    image_loader = torch.utils.data.DataLoader(\n","        image_dataset,\n","        batch_size=CFG.BATCH_SIZE,\n","        pin_memory=True,\n","        num_workers = CFG.NUM_WORKERS,\n","        drop_last=False\n","    )\n","\n","    embeds = []\n","    with torch.no_grad():\n","        for img,label in tqdm(image_loader): \n","            img = img.to(CFG.DEVICE)\n","            label = label.to(CFG.DEVICE)\n","            feat,_ = model(img,label)\n","            image_embeddings = feat.detach().cpu().numpy()\n","            embeds.append(image_embeddings)\n","    \n","    del model\n","    image_embeddings = np.concatenate(embeds)\n","    print(f'Our image embeddings shape is {image_embeddings.shape}')\n","    del embeds\n","    gc.collect()\n","    return image_embeddings"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def get_neighbors(df, embeddings, KNN = 50, image = True):\n","    '''\n","    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n","    '''\n","\n","    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n","    model.fit(embeddings)\n","    distances, indices = model.kneighbors(embeddings)\n","    \n","    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n","    if image:\n","        thresholds = list(np.arange(0.2,0.4,0.01))\n","    else:\n","        thresholds = list(np.arange(0.1, 1, 0.1))\n","    scores_f1 = []\n","    scores_recall = []\n","    scores_precision = []\n","    for threshold in thresholds:\n","        predictions = []\n","        for k in range(embeddings.shape[0]):\n","            idx = np.where(distances[k,] < threshold)[0]\n","            ids = indices[k,idx]\n","            posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n","            predictions.append(posting_ids)\n","\n","        df['pred_matches'] = predictions\n","\n","        df['f1'] = f1_score(df['matches'], df['pred_matches'])\n","        df['recall'] = recall_score(df['matches'], df['pred_matches'])\n","        df['precision'] = precision_score(df['matches'], df['pred_matches'])\n","\n","        score_f1 = df['f1'].mean()\n","        score_recall = df['recall'].mean()\n","        score_precision = df['precision'].mean()\n","\n","        print(f'Our f1 score for threshold {threshold} is {score_f1}, recall score is {score_recall}, mAP score is {score_precision}')\n","\n","        scores_f1.append(score_f1)\n","        scores_recall.append(score_recall)\n","        scores_precision.append(score_precision)\n","\n","    # create a dataframe to store threshold and scores\n","    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores_f1': scores_f1, 'scores_recall': scores_recall, 'scores_precision': scores_precision})\n","\n","    # obtain best f1 score\n","    max_score = thresholds_scores[thresholds_scores['scores_f1'] == thresholds_scores['scores_f1'].max()]\n","\n","    # obtain best threshold and scores\n","    best_threshold = max_score['thresholds'].values[0]\n","    best_score_f1 = max_score['scores_f1'].values[0]\n","    best_score_recall = max_score['scores_recall'].values[0]\n","    best_score_precision = max_score['scores_precision'].values[0]\n","\n","    print(f'Our best f1 score is {best_score_f1} and has a threshold {best_threshold}, corresponding recall score is {best_score_recall}, mAP score is {best_score_precision}')\n","\n","    # Use threshold\n","    predictions = []\n","    for k in range(embeddings.shape[0]):\n","        # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n","        if image:\n","            idx = np.where(distances[k,] < best_threshold)[0]\n","        else:\n","            idx = np.where(distances[k,] < best_threshold)[0]\n","        ids = indices[k,idx]\n","        posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n","        predictions.append(posting_ids)\n","        \n","    df['pred_matches'] = predictions\n","    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n","    df['recall'] = recall_score(df['matches'], df['pred_matches'])\n","    df['precision'] = precision_score(df['matches'], df['pred_matches'])\n","    \n","    del model, distances, indices\n","    gc.collect()\n","    return df, predictions"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def get_valid_neighbors(df, embeddings, KNN = 50, threshold = 0.36):\n","\n","    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n","    model.fit(embeddings)\n","    distances, indices = model.kneighbors(embeddings)\n","\n","    predictions = []\n","    for k in range(embeddings.shape[0]):\n","        idx = np.where(distances[k,] < threshold)[0]\n","        ids = indices[k,idx]\n","        posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n","        predictions.append(posting_ids)\n","        \n","    df['pred_matches'] = predictions\n","    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n","    df['recall'] = recall_score(df['matches'], df['pred_matches'])\n","    df['precision'] = precision_score(df['matches'], df['pred_matches'])\n","    \n","    del model, distances, indices\n","    gc.collect()\n","    return df, predictions"]},{"cell_type":"markdown","source":["# Training "],"metadata":{}},{"cell_type":"code","source":["def run_training():\n","    train_df, valid_df, test_df = read_dataset()\n","    train_dataset = ShopeeDataset(train_df, transform = get_train_transforms())\n","\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size = CFG.BATCH_SIZE,\n","        pin_memory = True,\n","        num_workers = CFG.NUM_WORKERS,\n","        shuffle = True,\n","        drop_last = True\n","    )\n","\n","    print(train_df['label_group'].nunique())\n","\n","    model = ShopeeModel()\n","    model = replace_activations(model, torch.nn.SiLU, Mish())\n","    model.to(CFG.DEVICE)\n","\n","    optimizer = Ranger(model.parameters(), lr = CFG.SCHEDULER_PARAMS['lr_start'])\n","    #optimizer = torch.optim.Adam(model.parameters(), lr = config.SCHEDULER_PARAMS['lr_start'])\n","    scheduler = ShopeeScheduler(optimizer,**CFG.SCHEDULER_PARAMS)\n","\n","    best_valid_f1 = 0.\n","    for i in range(CFG.EPOCHS):\n","        avg_loss_train = train_fn(model, train_dataloader, optimizer, scheduler, i)\n","\n","        valid_embeddings = get_valid_embeddings(valid_df, model)\n","        valid_df, valid_predictions = get_valid_neighbors(valid_df, valid_embeddings)\n","\n","        valid_f1 = valid_df.f1.mean()\n","        valid_recall = valid_df.recall.mean()\n","        valid_precision = valid_df.precision.mean()\n","        print(f'Valid f1 score = {valid_f1}, recall = {valid_recall}, precision = {valid_precision}')\n","\n","        if valid_f1 > best_valid_f1:\n","            best_valid_f1 = valid_f1\n","            print('Valid f1 score improved, model saved')\n","            torch.save(model.state_dict(),CFG.MODEL_PATH)\n","        \n","run_training()"],"metadata":{"trusted":true},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["6609\n","Building Model Backbone for densenet121 model\n","Using Curricular Face\n","Ranger optimizer loaded. \n","Gradient Centralization usage = True\n","GC applied to both conv and fc layers\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 1', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a950fbfad944ba78ce40608a97a0ed2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9188c0c2f6e4037a9f1dbd9160e5ad7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.12871044665550863, recall = 0.7623451152807135, precision = 0.07823039859833633\n","Valid f1 score improved, model saved\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 2', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50af8211f534919a10bdd2a3d1c266d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482dfe090c0d457d88990d522a15637b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.139042971087638, recall = 0.804562510073983, precision = 0.08540481271496012\n","Valid f1 score improved, model saved\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 3', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"538afc1307bb4d53bf8828a0c9daf059"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a1b6515823f46c38b7a3053f8bab7a6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.2065232153504527, recall = 0.8073942825174734, precision = 0.15356111531405237\n","Valid f1 score improved, model saved\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 4', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8f28b435b741c89f5d95ce7ebddec9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac68bc33005b4c40806a07325242ccaf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.3678873773486114, recall = 0.7819769155044686, precision = 0.32882865926309596\n","Valid f1 score improved, model saved\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 5', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e1dd26aa95427a8d96cb4d83943e65"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b2c646e4a7a4485971338575f1e6cda"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.5043754930704789, recall = 0.7686959926494046, precision = 0.4947960341065663\n","Valid f1 score improved, model saved\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 6', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322fca07b3d04ce58714e062258408f7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a10251058ae4f56a2fd269e9b635ca3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.5180775819087079, recall = 0.759039710725616, precision = 0.5217331917929521\n","Valid f1 score improved, model saved\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 7', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3c7d8168754cba84de79b43579b719"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5efac041841a46f88801b43ebe2c6d18"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.5080324176226996, recall = 0.7500275044187723, precision = 0.520651836604743\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 8', max=2568.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d582c654714556863cd27102a98311"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"323d5783e85e4124a28aca5b53b90297"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","Valid f1 score = 0.515162770433836, recall = 0.7468744482444063, precision = 0.5321278612151925\n"]}]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def get_test_embeddings(test_df):\n","    \n","    model = ShopeeModel()\n","    model.eval()\n","    model = replace_activations(model, torch.nn.SiLU, Mish())\n","    model.load_state_dict(torch.load(CFG.MODEL_PATH))\n","    model = model.to(CFG.DEVICE)\n","\n","    image_dataset = ShopeeImageDataset(test_df,transform=get_test_transforms())\n","    image_loader = torch.utils.data.DataLoader(\n","        image_dataset,\n","        batch_size=CFG.BATCH_SIZE,\n","        pin_memory=True,\n","        num_workers = CFG.NUM_WORKERS,\n","        drop_last=False\n","    )\n","\n","    embeds = []\n","    with torch.no_grad():\n","        for img,label in tqdm(image_loader): \n","            img = img.cuda()\n","            label = label.cuda()\n","            feat,_ = model(img,label)\n","            image_embeddings = feat.detach().cpu().numpy()\n","            embeds.append(image_embeddings)\n","    \n","    del model\n","    image_embeddings = np.concatenate(embeds)\n","    print(f'Our image embeddings shape is {image_embeddings.shape}')\n","    del embeds\n","    gc.collect()\n","    return image_embeddings"]},{"source":["## Best threshold Search"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Searching best threshold...\n","Building Model Backbone for densenet121 model\n","Using Curricular Face\n"]},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed3b952a19a4600aa04728f3e0134f2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6849, 6609)\n","threshold = 0.1 -> f1 score = 0.6749346231866037, recall = 0.5677316060188029, precision = 0.9953967995470404\n","threshold = 0.11 -> f1 score = 0.6827477815966936, recall = 0.5781338440908537, precision = 0.9943508852400661\n","threshold = 0.12 -> f1 score = 0.6899320672728741, recall = 0.5871430511465104, precision = 0.9935113845045949\n","threshold = 0.13 -> f1 score = 0.6965087340941419, recall = 0.5960913308477973, precision = 0.9919927943197764\n","threshold = 0.14 -> f1 score = 0.7034621933914499, recall = 0.6058310217428285, precision = 0.98965075903315\n","threshold = 0.15 -> f1 score = 0.7103014648492377, recall = 0.6162536231204555, precision = 0.986064559611567\n","threshold = 0.16 -> f1 score = 0.7150236274170823, recall = 0.6254488767802204, precision = 0.9810848009263178\n","threshold = 0.17 -> f1 score = 0.7208056486884042, recall = 0.6357184953008401, precision = 0.9766538153767299\n","threshold = 0.18 -> f1 score = 0.7264006371005716, recall = 0.6454214545335017, precision = 0.9727221087068934\n","threshold = 0.19 -> f1 score = 0.7322132430529167, recall = 0.6557992429822586, precision = 0.9679234959925462\n","threshold = 0.2 -> f1 score = 0.736723615287937, recall = 0.6663907897835142, precision = 0.9611797483861212\n","threshold = 0.21 -> f1 score = 0.7420217382425989, recall = 0.6775650717372301, precision = 0.9541738570748955\n","threshold = 0.22 -> f1 score = 0.7463510981524405, recall = 0.6898852674110302, precision = 0.9447692445977756\n","threshold = 0.23 -> f1 score = 0.7480021409794304, recall = 0.6995336870415425, precision = 0.9339389754775229\n","threshold = 0.24 -> f1 score = 0.7485295577645167, recall = 0.7094212352369281, precision = 0.9202644061244765\n","threshold = 0.25 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","threshold = 0.26 -> f1 score = 0.7473627967014828, recall = 0.7348373565319711, precision = 0.8824486261071528\n","threshold = 0.27 -> f1 score = 0.7417155678260602, recall = 0.7468939269402257, precision = 0.8585072735268461\n","threshold = 0.28 -> f1 score = 0.7330270596754656, recall = 0.7589980954412415, precision = 0.8291349753069825\n","threshold = 0.29 -> f1 score = 0.7208385481378958, recall = 0.7710700777412538, precision = 0.7954416914645347\n","threshold = 0.3 -> f1 score = 0.7007508624599814, recall = 0.7831729256765944, precision = 0.7504796177507504\n","threshold = 0.31 -> f1 score = 0.6735036479928873, recall = 0.7938511870697653, precision = 0.6981168166723067\n","threshold = 0.32 -> f1 score = 0.6378214482563601, recall = 0.8053468494908402, precision = 0.6364023933629581\n","threshold = 0.33 -> f1 score = 0.5930748748416853, recall = 0.8202444217178851, precision = 0.565666703326625\n","threshold = 0.34 -> f1 score = 0.5406479419113946, recall = 0.8307605042104625, precision = 0.49328772661666154\n","threshold = 0.35 -> f1 score = 0.48377528063160397, recall = 0.8430424198965887, precision = 0.41799836316958483\n","threshold = 0.36 -> f1 score = 0.42385547143812746, recall = 0.858649339765866, precision = 0.3462495328256995\n","threshold = 0.37 -> f1 score = 0.36360074961612815, recall = 0.8701699480460676, precision = 0.28144352730925637\n","threshold = 0.38 -> f1 score = 0.3072094259795239, recall = 0.8791995629691111, precision = 0.22578182898966973\n","threshold = 0.39 -> f1 score = 0.25994577432702304, recall = 0.8864293534315042, precision = 0.18280249646258795\n","threshold = 0.4 -> f1 score = 0.2217942954559136, recall = 0.8911929406423099, precision = 0.14920694404811546\n","threshold = 0.41 -> f1 score = 0.1967200425919999, recall = 0.8958194022634938, precision = 0.12841144473619442\n","threshold = 0.42 -> f1 score = 0.1802576015408975, recall = 0.8983422747597709, precision = 0.11531682291568865\n","threshold = 0.43 -> f1 score = 0.17069295702003848, recall = 0.899998373912578, precision = 0.10771650381627092\n","threshold = 0.44 -> f1 score = 0.16562932076416662, recall = 0.9007251267304085, precision = 0.10384101238964255\n","threshold = 0.45 -> f1 score = 0.16290386534248782, recall = 0.9010481911842304, precision = 0.10171275284860384\n","threshold = 0.46 -> f1 score = 0.16136378495685916, recall = 0.9011965752756436, precision = 0.1005097659908781\n","threshold = 0.47 -> f1 score = 0.1605672091507407, recall = 0.9013998857031922, precision = 0.09986419743431983\n","threshold = 0.48 -> f1 score = 0.15998598920508497, recall = 0.9015169688392232, precision = 0.09937088498671894\n","threshold = 0.49 -> f1 score = 0.1595470143340035, recall = 0.901535808415521, precision = 0.09898477956908773\n","Best threshold = 0.25\n","Best f1 score = 0.748850817398089\n"]}],"source":["train_df, valid_df, test_df = read_dataset()\n","\n","print(\"Searching best threshold...\")\n","search_space = np.arange(10, 50, 1)\n","\n","model = ShopeeModel()\n","model.eval()\n","model = replace_activations(model, torch.nn.SiLU, Mish())\n","model.load_state_dict(torch.load(CFG.MODEL_PATH))\n","model = model.to(CFG.DEVICE)\n","\n","valid_embeddings = get_valid_embeddings(valid_df, model)\n","\n","best_f1_valid = 0.\n","best_threshold = 0.\n","\n","for i in search_space:\n","    threshold = i / 100\n","    valid_df, valid_predictions = get_valid_neighbors(valid_df, valid_embeddings, threshold=threshold)\n","\n","    valid_f1 = valid_df.f1.mean()\n","    valid_recall = valid_df.recall.mean()\n","    valid_precision = valid_df.precision.mean()\n","\n","    print(f\"threshold = {threshold} -> f1 score = {valid_f1}, recall = {valid_recall}, precision = {valid_precision}\")\n","\n","    if (valid_f1 > best_f1_valid):\n","        best_f1_valid = valid_f1\n","        best_threshold = threshold\n","\n","print(\"Best threshold =\", best_threshold)\n","print(\"Best f1 score =\", best_f1_valid)\n","BEST_THRESHOLD = best_threshold"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Searching best knn...\n","knn = 40 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 42 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 44 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 46 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 48 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 50 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 52 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 54 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 56 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 58 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 60 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 62 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 64 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 66 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 68 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 70 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 72 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 74 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 76 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","knn = 78 -> f1 score = 0.748850817398089, recall = 0.7211400983951249, precision = 0.9037040765635327\n","Best knn = 40\n","Best f1 score = 0.748850817398089\n"]}],"source":["print(\"Searching best knn...\")\n","\n","search_space = np.arange(40, 80, 2)\n","\n","best_f1_valid = 0.\n","best_knn = 0\n","\n","for knn in search_space:\n","\n","    valid_df, valid_predictions = get_valid_neighbors(valid_df, valid_embeddings, KNN=knn, threshold=BEST_THRESHOLD)\n","\n","    valid_f1 = valid_df.f1.mean()\n","    valid_recall = valid_df.recall.mean()\n","    valid_precision = valid_df.precision.mean()\n","\n","    print(f\"knn = {knn} -> f1 score = {valid_f1}, recall = {valid_recall}, precision = {valid_precision}\")\n","\n","    if (valid_f1 > best_f1_valid):\n","        best_f1_valid = valid_f1\n","        best_knn = knn\n","\n","print(\"Best knn =\", best_knn)\n","print(\"Best f1 score =\", best_f1_valid)\n","BEST_KNN = best_knn"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=857.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1754e0ffa8e7474ebced5a8aaba7ee04"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Our image embeddings shape is (6851, 6609)\n","Test f1 score = 0.7464257057378139, recall = 0.7160501321775866, precision = 0.904700830566076\n"]}],"source":["test_embeddings = get_valid_embeddings(test_df,model)\n","test_df, test_predictions = get_valid_neighbors(test_df, test_embeddings, KNN = BEST_KNN, threshold = BEST_THRESHOLD)\n","\n","test_f1 = test_df.f1.mean()\n","test_recall = test_df.recall.mean()\n","test_precision = test_df.precision.mean()\n","print(f'Test f1 score = {test_f1}, recall = {test_recall}, precision = {test_precision}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}