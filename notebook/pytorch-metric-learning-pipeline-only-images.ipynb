{"metadata":{"kernelspec":{"name":"python383jvsc74a57bd00b64f3f517ef2f38123c9b9d844dc7ba7aeffcc4559b7061ceea5f8a66fe5b86","display_name":"Python 3.8.3 64-bit ('base': conda)","language":"python"},"language_info":{"name":"python","version":"3.8.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# About this Notebook\n","\n","Ragnar [here](https://www.kaggle.com/ragnar123/shopee-efficientnetb3-arcmarginproduct) has showed us how to train efficientb3 with arcface and CrossEntropy Loss\n","\n","With this notebook here you can train any efficientnet b0-b7 with following three Metric Learning techniques above the Cross-Entropy Loss:\n","* ArcFace : Most Popular in this competition \n","* CosFace : https://arxiv.org/abs/1801.09414\n","* Adacos : https://arxiv.org/abs/1905.00292\n","\n","Don't worry if these terms feel alien to you , following are few resources to understand them:\n","* https://www.kaggle.com/c/shopee-product-matching/discussion/226279 --- Beautiful Explanantion by Chris Deotte\n","* https://www.kaggle.com/slawekbiel/arcface-explained --  Beautiful kernel explaining ArcFace\n","\n","\n","The training strategy used is as follows:\n","* Make 5-folds stratified on label groups\n","* Use one of the metric learning losses to predict the label  groups using cross entropy loss\n","\n","This notebook basically just converts ragnar's notebook into your beloved Pytorch . <b>Inference notebook coming soon</b> \n","<br>I have quick run this notebook because I have exhausted my gpu quota for this week . To see how this pipeline runs you can fork and run it .\n","\n","Hope this helps\n","\n","### Techniques planned to be added\n","\n","* Multiface posted by Mobassir [here](https://www.kaggle.com/c/shopee-product-matching/discussion/227383)\n","\n","You can also have a look at Siamese Type Training Example [here](https://www.kaggle.com/tanulsingh077/siamese-style-training-efficient-net-b0-on-tpu-s)"],"metadata":{}},{"cell_type":"code","source":["import sys\n","sys.path.insert(0,'/home/yjx/project/input/pytorch-image-models')"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["# Preliminaries\n","from tqdm import tqdm\n","import math\n","import random\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Visuals and CV2\n","import cv2\n","\n","# albumentations for augs\n","import albumentations\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","#torch\n","import timm\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset,DataLoader\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau"],"metadata":{"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":["# Configuration"],"metadata":{}},{"cell_type":"code","source":["DIM = (512,512)\n","\n","NUM_WORKERS = 4\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 8\n","EPOCHS = 30\n","SEED = 2020\n","LR = 3e-4\n","\n","device = torch.device('cuda')\n","\n","\n","################################################# MODEL ####################################################################\n","\n","model_name = 'efficientnet_b0' #efficientnet_b0-b7\n","\n","################################################ Metric Loss and its params #######################################################\n","loss_module = 'arcface' #'cosface' #'adacos'\n","s = 30.0\n","m = 0.5 \n","ls_eps = 0.0\n","easy_margin = False\n","\n","\n","####################################### Scheduler and its params ############################################################\n","SCHEDULER = 'CosineAnnealingWarmRestarts' #'CosineAnnealingLR'\n","factor=0.2 # ReduceLROnPlateau\n","patience=4 # ReduceLROnPlateau\n","eps=1e-6 # ReduceLROnPlateau\n","T_max=10 # CosineAnnealingLR\n","T_0=4 # CosineAnnealingWarmRestarts\n","min_lr=1e-6\n","\n","############################################## Model Params ###############################################################\n","model_params = {\n","    'n_classes':11014,\n","    'model_name':'efficientnet_b0',\n","    'use_fc':False,\n","    'fc_dim':512,\n","    'dropout':0.0,\n","    'loss_module':loss_module,\n","    's':30.0,\n","    'margin':0.50,\n","    'ls_eps':0.0,\n","    'theta_zero':0.785,\n","    'pretrained':True\n","}\n"],"metadata":{"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{}},{"cell_type":"code","source":["def seed_torch(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_torch(SEED)"],"metadata":{"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    \n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":["def fetch_scheduler(optimizer):\n","        if SCHEDULER =='ReduceLROnPlateau':\n","            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True, eps=eps)\n","        elif SCHEDULER =='CosineAnnealingLR':\n","            scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=min_lr, last_epoch=-1)\n","        elif SCHEDULER =='CosineAnnealingWarmRestarts':\n","            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=1, eta_min=min_lr, last_epoch=-1)\n","        return scheduler"],"metadata":{"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["def fetch_loss():\n","    loss = nn.CrossEntropyLoss()\n","    return loss"],"metadata":{"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":["# Augmentations"],"metadata":{}},{"cell_type":"code","source":["def get_train_transforms():\n","    return albumentations.Compose(\n","        [   \n","            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n","            albumentations.HorizontalFlip(p=0.5),\n","            albumentations.VerticalFlip(p=0.5),\n","            albumentations.Rotate(limit=120, p=0.8),\n","            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n","            #albumentations.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n","            #albumentations.ShiftScaleRotate(\n","              #  shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n","            #),\n","            albumentations.Normalize(),\n","            ToTensorV2(p=1.0),\n","        ]\n","    )\n","\n","def get_valid_transforms():\n","\n","    return albumentations.Compose(\n","        [\n","            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n","            albumentations.Normalize(),\n","        ToTensorV2(p=1.0)\n","        ]\n","    )"],"metadata":{"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{}},{"cell_type":"code","source":["class ShopeeDataset(Dataset):\n","    def __init__(self, csv, transforms=None):\n","\n","        self.csv = csv.reset_index()\n","        self.augmentations = transforms\n","\n","    def __len__(self):\n","        return self.csv.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.csv.iloc[index]\n","        \n","        text = row.title\n","        \n","        image = cv2.imread(row.filepath)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        \n","        if self.augmentations:\n","            augmented = self.augmentations(image=image)\n","            image = augmented['image']       \n","        \n","        \n","        return image,torch.tensor(row.label_group)"],"metadata":{"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{}},{"cell_type":"code","source":["class ShopeeNet(nn.Module):\n","\n","    def __init__(self,\n","                 n_classes,\n","                 model_name='efficientnet_b0',\n","                 use_fc=False,\n","                 fc_dim=512,\n","                 dropout=0.0,\n","                 loss_module='softmax',\n","                 s=30.0,\n","                 margin=0.50,\n","                 ls_eps=0.0,\n","                 theta_zero=0.785,\n","                 pretrained=True):\n","        \"\"\"\n","        :param n_classes:\n","        :param model_name: name of model from pretrainedmodels\n","            e.g. resnet50, resnext101_32x4d, pnasnet5large\n","        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n","        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n","        \"\"\"\n","        super(ShopeeNet, self).__init__()\n","        print('Building Model Backbone for {} model'.format(model_name))\n","\n","        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n","        final_in_features = self.backbone.classifier.in_features\n","        \n","        self.backbone.classifier = nn.Identity()\n","        self.backbone.global_pool = nn.Identity()\n","        \n","        self.pooling =  nn.AdaptiveAvgPool2d(1)\n","            \n","        self.use_fc = use_fc\n","        if use_fc:\n","            self.dropout = nn.Dropout(p=dropout)\n","            self.fc = nn.Linear(final_in_features, fc_dim)\n","            self.bn = nn.BatchNorm1d(fc_dim)\n","            self._init_params()\n","            final_in_features = fc_dim\n","\n","        self.loss_module = loss_module\n","        if loss_module == 'arcface':\n","            self.final = ArcMarginProduct(final_in_features, n_classes,\n","                                          s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n","        elif loss_module == 'cosface':\n","            self.final = AddMarginProduct(final_in_features, n_classes, s=s, m=margin)\n","        elif loss_module == 'adacos':\n","            self.final = AdaCos(final_in_features, n_classes, m=margin, theta_zero=theta_zero)\n","        else:\n","            self.final = nn.Linear(final_in_features, n_classes)\n","\n","    def _init_params(self):\n","        nn.init.xavier_normal_(self.fc.weight)\n","        nn.init.constant_(self.fc.bias, 0)\n","        nn.init.constant_(self.bn.weight, 1)\n","        nn.init.constant_(self.bn.bias, 0)\n","\n","    def forward(self, x, label):\n","        feature = self.extract_feat(x)\n","        if self.loss_module in ('arcface', 'cosface', 'adacos'):\n","            logits = self.final(feature, label)\n","        else:\n","            logits = self.final(feature)\n","        return logits\n","\n","    def extract_feat(self, x):\n","        batch_size = x.shape[0]\n","        x = self.backbone(x)\n","        x = self.pooling(x).view(batch_size, -1)\n","\n","        if self.use_fc:\n","            x = self.dropout(x)\n","            x = self.fc(x)\n","            x = self.bn(x)\n","\n","        return x"],"metadata":{"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":["# Metric Learning Losses\n","\n","* https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py -- Code Taken from here"],"metadata":{}},{"cell_type":"code","source":["class AdaCos(nn.Module):\n","    def __init__(self, in_features, out_features, m=0.50, ls_eps=0, theta_zero=math.pi/4):\n","        super(AdaCos, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.theta_zero = theta_zero\n","        self.s = math.log(out_features - 1) / math.cos(theta_zero)\n","        self.m = m\n","        self.ls_eps = ls_eps  # label smoothing\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","    def forward(self, input, label):\n","        # normalize features\n","        x = F.normalize(input)\n","        # normalize weights\n","        W = F.normalize(self.weight)\n","        # dot product\n","        logits = F.linear(x, W)\n","        # add margin\n","        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n","        target_logits = torch.cos(theta + self.m)\n","        one_hot = torch.zeros_like(logits)\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        if self.ls_eps > 0:\n","            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n","        output = logits * (1 - one_hot) + target_logits * one_hot\n","        # feature re-scale\n","        with torch.no_grad():\n","            B_avg = torch.where(one_hot < 1, torch.exp(self.s * logits), torch.zeros_like(logits))\n","            B_avg = torch.sum(B_avg) / input.size(0)\n","            theta_med = torch.median(theta)\n","            self.s = torch.log(B_avg) / torch.cos(torch.min(self.theta_zero * torch.ones_like(theta_med), theta_med))\n","        output *= self.s\n","\n","        return output"],"metadata":{"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["class ArcMarginProduct(nn.Module):\n","    r\"\"\"Implement of large margin arc distance: :\n","        Args:\n","            in_features: size of each input sample\n","            out_features: size of each output sample\n","            s: norm of input feature\n","            m: margin\n","            cos(theta + m)\n","        \"\"\"\n","    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n","        super(ArcMarginProduct, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.s = s\n","        self.m = m\n","        self.ls_eps = ls_eps  # label smoothing\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","        self.easy_margin = easy_margin\n","        self.cos_m = math.cos(m)\n","        self.sin_m = math.sin(m)\n","        self.th = math.cos(math.pi - m)\n","        self.mm = math.sin(math.pi - m) * m\n","\n","    def forward(self, input, label):\n","        # --------------------------- cos(theta) & phi(theta) ---------------------------\n","        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n","        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n","        phi = cosine * self.cos_m - sine * self.sin_m\n","        if self.easy_margin:\n","            phi = torch.where(cosine > 0, phi, cosine)\n","        else:\n","            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n","        # --------------------------- convert label to one-hot ---------------------------\n","        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n","        one_hot = torch.zeros(cosine.size(), device='cuda')\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        if self.ls_eps > 0:\n","            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n","        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n","        output *= self.s\n","\n","        return output"],"metadata":{"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":["class AddMarginProduct(nn.Module):\n","    r\"\"\"Implement of large margin cosine distance: :\n","    Args:\n","        in_features: size of each input sample\n","        out_features: size of each output sample\n","        s: norm of input feature\n","        m: margin\n","        cos(theta) - m\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n","        super(AddMarginProduct, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.s = s\n","        self.m = m\n","        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n","        nn.init.xavier_uniform_(self.weight)\n","\n","    def forward(self, input, label):\n","        # --------------------------- cos(theta) & phi(theta) ---------------------------\n","        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n","        phi = cosine - self.m\n","        # --------------------------- convert label to one-hot ---------------------------\n","        one_hot = torch.zeros(cosine.size(), device='cuda')\n","        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n","        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n","        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n","        output *= self.s\n","        # print(output)\n","\n","        return output"],"metadata":{"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":["# Training Function"],"metadata":{}},{"cell_type":"code","source":["def train_fn(dataloader,model,criterion,optimizer,device,scheduler,epoch):\n","    model.train()\n","    loss_score = AverageMeter()\n","    \n","    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for bi,d in tk0:\n","        \n","        batch_size = d[0].shape[0]\n","\n","\n","        images = d[0]\n","        targets = d[1]\n","\n","        images = images.to(device)\n","        targets = targets.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(images,targets)\n","        \n","        loss = criterion(output,targets)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        loss_score.update(loss.detach().item(), batch_size)\n","        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch,LR=optimizer.param_groups[0]['lr'])\n","        \n","    if scheduler is not None:\n","            scheduler.step()\n","        \n","    return loss_score"],"metadata":{"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Function"],"metadata":{}},{"cell_type":"code","source":["def eval_fn(data_loader,model,criterion,device):\n","    \n","    loss_score = AverageMeter()\n","    \n","    model.eval()\n","    tk0 = tqdm(enumerate(data_loader), total=len(data_loader))\n","    \n","    with torch.no_grad():\n","        \n","        for bi,d in tk0:\n","            batch_size = d[0].size()[0]\n","\n","            image = d[0]\n","            targets = d[1]\n","\n","            image = image.to(device)\n","            targets = targets.to(device)\n","\n","            output = model(image,targets)\n","\n","            loss = criterion(output,targets)\n","            \n","            loss_score.update(loss.detach().item(), batch_size)\n","            tk0.set_postfix(Eval_Loss=loss_score.avg)\n","            \n","    return loss_score"],"metadata":{"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":["# Engine"],"metadata":{}},{"cell_type":"code","source":["data = pd.read_csv('../input/shopee-folds/folds.csv')\n","data['filepath'] = data['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/', 'train_images', x))"],"metadata":{"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"trusted":true},"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         posting_id                                 image       image_phash  \\\n","0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n","1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n","2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n","3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n","4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n","\n","                                               title  label_group  fold  \\\n","0                          Paper Bag Victoria Secret    249114794     3   \n","1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045     3   \n","2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891     4   \n","3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188     3   \n","4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069     1   \n","\n","                                            filepath  \n","0  ../input/shopee-product-matching/train_images/...  \n","1  ../input/shopee-product-matching/train_images/...  \n","2  ../input/shopee-product-matching/train_images/...  \n","3  ../input/shopee-product-matching/train_images/...  \n","4  ../input/shopee-product-matching/train_images/...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>fold</th>\n      <th>filepath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>3</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>3</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>4</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>3</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>1</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":137}]},{"cell_type":"code","source":["encoder = LabelEncoder()\n","data['label_group'] = encoder.fit_transform(data['label_group'])"],"metadata":{"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["def run():\n","        \n","    train = data[data['fold']!=0].reset_index(drop=True)\n","    valid = data[data['fold']==0].reset_index(drop=True)\n","    # Defining DataSet\n","    train_dataset = ShopeeDataset(\n","        csv=train,\n","        transforms=get_train_transforms(),\n","    )\n","        \n","    valid_dataset = ShopeeDataset(\n","        csv=valid,\n","        transforms=get_valid_transforms(),\n","    )\n","        \n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=TRAIN_BATCH_SIZE,\n","        pin_memory=True,\n","        drop_last=True,\n","        num_workers=NUM_WORKERS\n","    )\n","    \n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=VALID_BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        shuffle=False,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","    \n","    # Defining Device\n","    device = torch.device(\"cuda\")\n","    \n","    # Defining Model for specific fold\n","    model = ShopeeNet(**model_params)\n","    model.to(device)\n","    \n","    #DEfining criterion\n","    criterion = fetch_loss()\n","    criterion.to(device)\n","        \n","    # Defining Optimizer with weight decay to params other than bias and layer norms\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","            ]  \n","    \n","    optimizer = torch.optim.Adam(optimizer_parameters, lr=LR)\n","    \n","    #Defining LR SCheduler\n","    scheduler = fetch_scheduler(optimizer)\n","        \n","    # THE ENGINE LOOP\n","    best_loss = 10000\n","    for epoch in range(EPOCHS):\n","        train_loss = train_fn(train_loader, model,criterion, optimizer, device,scheduler=scheduler,epoch=epoch)\n","        \n","        valid_loss = eval_fn(valid_loader, model, criterion,device)\n","        \n","        if valid_loss.avg < best_loss:\n","            best_loss = valid_loss.avg\n","            torch.save(model.state_dict(),f'model_{model_name}_IMG_SIZE_{DIM[0]}_{loss_module}.bin')\n","            print('best model found for epoch {}'.format(epoch))"],"metadata":{"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["run()"],"metadata":{"trusted":true},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Building Model Backbone for efficientnet_b0 model\n","Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth\" to /home/yjx/.cache/torch/hub/checkpoints/efficientnet_b0_ra-3dd342df.pth\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 2.00 MiB (GPU 1; 9.78 GiB total capacity; 3.62 GiB already allocated; 2.81 MiB free; 3.63 GiB reserved in total by PyTorch)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-120-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-119-219f7986692b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Defining Model for specific fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShopeeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#DEfining criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/home/caslx/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 1; 9.78 GiB total capacity; 3.62 GiB already allocated; 2.81 MiB free; 3.63 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}