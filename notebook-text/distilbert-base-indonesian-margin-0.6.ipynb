{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legislative-lyric",
   "metadata": {
    "papermill": {
     "duration": 0.01723,
     "end_time": "2021-05-18T15:03:54.207828",
     "exception": false,
     "start_time": "2021-05-18T15:03:54.190598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shopee Training distilbert-base-indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "choice-cause",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:54.245068Z",
     "iopub.status.busy": "2021-05-18T15:03:54.243603Z",
     "iopub.status.idle": "2021-05-18T15:03:54.253166Z",
     "shell.execute_reply": "2021-05-18T15:03:54.252613Z"
    },
    "papermill": {
     "duration": 0.029285,
     "end_time": "2021-05-18T15:03:54.253280",
     "exception": false,
     "start_time": "2021-05-18T15:03:54.223995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 15:03:54.248688\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "start_time = time.time()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bigger-daily",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:54.291057Z",
     "iopub.status.busy": "2021-05-18T15:03:54.290553Z",
     "iopub.status.idle": "2021-05-18T15:03:57.158572Z",
     "shell.execute_reply": "2021-05-18T15:03:57.157702Z"
    },
    "papermill": {
     "duration": 2.888732,
     "end_time": "2021-05-18T15:03:57.158717",
     "exception": false,
     "start_time": "2021-05-18T15:03:54.269985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "several-fifteen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.258727Z",
     "iopub.status.busy": "2021-05-18T15:03:57.258010Z",
     "iopub.status.idle": "2021-05-18T15:03:57.262087Z",
     "shell.execute_reply": "2021-05-18T15:03:57.261692Z"
    },
    "papermill": {
     "duration": 0.086498,
     "end_time": "2021-05-18T15:03:57.262198",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.175700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CSV = '../input/shopee-product-matching/train.csv'\n",
    "\n",
    "class CFG:\n",
    "    \n",
    "    compute_cv = True  # set False to train model for submission\n",
    "\n",
    "    ### BERT\n",
    "#     bert_model_name = '../input/bertmodel/paraphrase-xlm-r-multilingual-v1'\n",
    "    bert_model_name = '../input/bertmodel/distilbert-base-indonesian'\n",
    "#     bert_model_name = '../input/bertmodel/roberta-base' # very bad performance\n",
    "#     bert_model_name = '../input/bertmodel/paraphrase-distilroberta-base-v1'\n",
    "#     bert_model_name = '../input/bert-model-pretrained/bert-base-multilingual-uncased/bert-base-multilingual-uncased'\n",
    "#     bert_model_name = '../input/bert-model-pretrained/bert-base-indonesian-1.5G/bert-base-indonesian-1.5G'\n",
    "\n",
    "    max_length = 128\n",
    "\n",
    "    ### ArcFace\n",
    "    scale = 30\n",
    "    margin = 0.6\n",
    "    fc_dim = 768\n",
    "    seed = 412\n",
    "    classes = 11014\n",
    "    \n",
    "    # groupkfold\n",
    "    N_SPLITS = 5\n",
    "    TEST_FOLD = 0\n",
    "    VALID_FOLD = 1\n",
    "    \n",
    "    ### Training\n",
    "    batch_size = 16\n",
    "    accum_iter = 1  # 1 if use_sam = True\n",
    "    epochs = 8\n",
    "    min_save_epoch = epochs // 3\n",
    "    use_sam = True  # SAM (Sharpness-Aware Minimization for Efficiently Improving Generalization)\n",
    "    use_amp = True  # Automatic Mixed Precision\n",
    "    num_workers = 2  # On Windows, set 0 or export train_fn and TitleDataset as .py files for faster training.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    \n",
    "    ### NearestNeighbors\n",
    "    bert_knn = 50\n",
    "    bert_knn_threshold = 0.4  # Cosine distance threshold\n",
    "    \n",
    "    ### GradualWarmupSchedulerV2（lr_start -> lr_max -> lr_min）\n",
    "    scheduler_params = {\n",
    "        \"lr_start\": 7.5e-6,\n",
    "        \"lr_max\": 1e-4,\n",
    "        \"lr_min\": 2.74e-5, # 1.5e-5,\n",
    "    }\n",
    "    multiplier = scheduler_params['lr_max'] / scheduler_params['lr_start']\n",
    "    eta_min = scheduler_params['lr_min']  # last minimum learning rate\n",
    "    freeze_epo = 0\n",
    "    warmup_epo = 2\n",
    "    cosine_epo = epochs - freeze_epo - warmup_epo\n",
    "    \n",
    "    ### save_model_path\n",
    "    save_model_path = f\"./{bert_model_name.rsplit('/', 1)[-1]}_epoch{epochs}-bs{batch_size}x{accum_iter}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "earned-trader",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.301019Z",
     "iopub.status.busy": "2021-05-18T15:03:57.300394Z",
     "iopub.status.idle": "2021-05-18T15:03:57.305701Z",
     "shell.execute_reply": "2021-05-18T15:03:57.305237Z"
    },
    "papermill": {
     "duration": 0.026661,
     "end_time": "2021-05-18T15:03:57.305804",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.279143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True # set True to be faster\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-grocery",
   "metadata": {
    "papermill": {
     "duration": 0.017362,
     "end_time": "2021-05-18T15:03:57.340185",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.322823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generous-roulette",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.381270Z",
     "iopub.status.busy": "2021-05-18T15:03:57.380077Z",
     "iopub.status.idle": "2021-05-18T15:03:57.382777Z",
     "shell.execute_reply": "2021-05-18T15:03:57.382332Z"
    },
    "papermill": {
     "duration": 0.025724,
     "end_time": "2021-05-18T15:03:57.382873",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.357149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Dataset\n",
    "\n",
    "class TitleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, text_column, label_column):\n",
    "        texts = df[text_column]\n",
    "        self.labels = df[label_column].values\n",
    "        \n",
    "        self.titles = []\n",
    "        for title in texts:\n",
    "            title = title.encode('utf-8').decode(\"unicode_escape\")\n",
    "            title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n",
    "            title = title.lower()\n",
    "            self.titles.append(title)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.titles[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broadband-hollow",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.428518Z",
     "iopub.status.busy": "2021-05-18T15:03:57.424193Z",
     "iopub.status.idle": "2021-05-18T15:03:57.430862Z",
     "shell.execute_reply": "2021-05-18T15:03:57.430458Z"
    },
    "papermill": {
     "duration": 0.031167,
     "end_time": "2021-05-18T15:03:57.430959",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.399792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### SAM Optimizer 2020/1/16\n",
    "# https://github.com/davda54/sam/blob/main/sam.py\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fluid-disability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.478601Z",
     "iopub.status.busy": "2021-05-18T15:03:57.478005Z",
     "iopub.status.idle": "2021-05-18T15:03:57.481588Z",
     "shell.execute_reply": "2021-05-18T15:03:57.481166Z"
    },
    "papermill": {
     "duration": 0.03375,
     "end_time": "2021-05-18T15:03:57.481683",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.447933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GradualWarmupScheduler\n",
    "# https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
    "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n",
    "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
    "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        if self.multiplier < 1.:\n",
    "            raise ValueError('multiplier should be greater thant or equal to 1.')\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        super(GradualWarmupScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_last_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
    "        if self.last_epoch <= self.total_epoch:\n",
    "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            if epoch is None:\n",
    "                self.after_scheduler.step(metrics, None)\n",
    "            else:\n",
    "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
    "\n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
    "            if self.finished and self.after_scheduler:\n",
    "                if epoch is None:\n",
    "                    self.after_scheduler.step(None)\n",
    "                else:\n",
    "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
    "                self._last_lr = self.after_scheduler.get_last_lr()\n",
    "            else:\n",
    "                return super(GradualWarmupScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.step_ReduceLROnPlateau(metrics, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apparent-ballet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.523730Z",
     "iopub.status.busy": "2021-05-18T15:03:57.522531Z",
     "iopub.status.idle": "2021-05-18T15:03:57.524785Z",
     "shell.execute_reply": "2021-05-18T15:03:57.525200Z"
    },
    "papermill": {
     "duration": 0.026308,
     "end_time": "2021-05-18T15:03:57.525309",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.499001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GradualWarmupSchedulerV2\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "english-flush",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.572176Z",
     "iopub.status.busy": "2021-05-18T15:03:57.571658Z",
     "iopub.status.idle": "2021-05-18T15:03:57.574607Z",
     "shell.execute_reply": "2021-05-18T15:03:57.574178Z"
    },
    "papermill": {
     "duration": 0.03204,
     "end_time": "2021-05-18T15:03:57.574704",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.542664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Train one epoch\n",
    "\n",
    "def train_fn(model, data_loader, optimizer, scheduler, use_sam, accum_iter, epoch, device, use_amp):\n",
    "    model.train()\n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    fin_loss = 0.0\n",
    "    tk = tqdm(data_loader, desc = \"Training epoch: \" + str(epoch+1), ncols=100)\n",
    "\n",
    "    for t, (texts, labels) in enumerate(tk):\n",
    "        texts = list(texts)\n",
    "\n",
    "        if use_sam:\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    _, loss = model(texts, labels)\n",
    "                loss.mean().backward()\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "                fin_loss += loss.item() \n",
    "                with torch.cuda.amp.autocast():\n",
    "                     _, loss_second = model(texts, labels)\n",
    "                loss_second.mean().backward()\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                _, loss = model(texts, labels)\n",
    "                loss.mean().backward()\n",
    "                optimizer.first_step(zero_grad=True)\n",
    "                fin_loss += loss.item() \n",
    "                _, loss_second = model(texts, labels)\n",
    "                loss_second.mean().backward()\n",
    "                optimizer.second_step(zero_grad=True)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        else:  # if use_sam == False\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    _, loss = model(texts, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                fin_loss += loss.item() \n",
    "                # mini-batch accumulation\n",
    "                if (t + 1) % accum_iter == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                _, loss = model(texts, labels)\n",
    "                loss.backward()\n",
    "                fin_loss += loss.item() \n",
    "                # mini-batch accumulation\n",
    "                if (t + 1) % accum_iter == 0:\n",
    "                    optimizer.step() \n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    scheduler.step()\n",
    "    return model, fin_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "julian-battle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.622149Z",
     "iopub.status.busy": "2021-05-18T15:03:57.620964Z",
     "iopub.status.idle": "2021-05-18T15:03:57.623661Z",
     "shell.execute_reply": "2021-05-18T15:03:57.623240Z"
    },
    "papermill": {
     "duration": 0.031724,
     "end_time": "2021-05-18T15:03:57.623761",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.592037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Validation\n",
    "\n",
    "def getMetric(col):\n",
    "    def f1score(row):\n",
    "        n = len(np.intersect1d(row.target, row[col]))\n",
    "        return 2 * n / (len(row.target) + len(row[col]))\n",
    "    return f1score\n",
    "\n",
    "\n",
    "def get_bert_embeddings(df, column, model, chunk=32):\n",
    "    model.eval()\n",
    "    \n",
    "    bert_embeddings = torch.zeros((df.shape[0], 768)).to(CFG.device)\n",
    "    for i in tqdm(list(range(0, df.shape[0], chunk)) + [df.shape[0]-chunk], desc=\"get_bert_embeddings\", ncols=80):\n",
    "        titles = []\n",
    "        for title in df[column][i : i + chunk].values:\n",
    "            try:\n",
    "                title = title.encode('utf-8').decode(\"unicode_escape\")\n",
    "                title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n",
    "            except:\n",
    "                pass\n",
    "            #title = text_punctuation(title)\n",
    "            title = title.lower()\n",
    "            titles.append(title)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            if CFG.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    model_output = model(titles)\n",
    "            else:\n",
    "                model_output = model(titles)\n",
    "            \n",
    "        bert_embeddings[i : i + chunk] = model_output\n",
    "    \n",
    "    del model, titles, model_output\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return bert_embeddings\n",
    "\n",
    "\n",
    "def get_neighbors(df, embeddings, knn=50, threshold=0.0):\n",
    "\n",
    "    model = NearestNeighbors(n_neighbors=knn, metric='cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    preds = []\n",
    "    for k in range(embeddings.shape[0]):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = df['posting_id'].iloc[ids].values\n",
    "        preds.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effective-mongolia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.670054Z",
     "iopub.status.busy": "2021-05-18T15:03:57.669454Z",
     "iopub.status.idle": "2021-05-18T15:03:57.672621Z",
     "shell.execute_reply": "2021-05-18T15:03:57.672198Z"
    },
    "papermill": {
     "duration": 0.031455,
     "end_time": "2021-05-18T15:03:57.672717",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.641262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ArcFace\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "                \n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        if CFG.use_amp:\n",
    "            cosine = F.linear(F.normalize(input), F.normalize(self.weight)).float()  # if CFG.use_amp\n",
    "        else:\n",
    "            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "        return output, self.criterion(output,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caroline-jenny",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.723783Z",
     "iopub.status.busy": "2021-05-18T15:03:57.723056Z",
     "iopub.status.idle": "2021-05-18T15:03:57.730118Z",
     "shell.execute_reply": "2021-05-18T15:03:57.729734Z"
    },
    "papermill": {
     "duration": 0.039796,
     "end_time": "2021-05-18T15:03:57.730214",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.690418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### BERT\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "class ShopeeBertModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = CFG.classes,\n",
    "        model_name = CFG.bert_model_name,\n",
    "        fc_dim = CFG.fc_dim,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = True\n",
    "    ):\n",
    "\n",
    "        super(ShopeeBertModel,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name).to(CFG.device)\n",
    "\n",
    "        in_features = 768\n",
    "        self.use_fc = use_fc\n",
    "        \n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=0.0)\n",
    "            self.classifier = nn.Linear(in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            in_features = fc_dim\n",
    "            \n",
    "        self.final = ArcMarginProduct(\n",
    "            in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, texts, labels=torch.tensor([0])):\n",
    "        features = self.extract_features(texts)\n",
    "        if self.training:\n",
    "            logits = self.final(features, labels.to(CFG.device))\n",
    "            return logits\n",
    "        else:\n",
    "            return features\n",
    "        \n",
    "    def extract_features(self, texts):\n",
    "        encoding = self.tokenizer(texts, padding=True, truncation=True,\n",
    "                             max_length=CFG.max_length, return_tensors='pt').to(CFG.device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        embedding = self.backbone(input_ids, attention_mask=attention_mask)\n",
    "        x = mean_pooling(embedding, attention_mask)\n",
    "        \n",
    "        if self.use_fc and self.training:\n",
    "            x = self.dropout(x)\n",
    "            x = self.classifier(x)\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-berkeley",
   "metadata": {
    "papermill": {
     "duration": 0.017578,
     "end_time": "2021-05-18T15:03:57.766047",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.748469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hybrid-ordinance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:57.815050Z",
     "iopub.status.busy": "2021-05-18T15:03:57.814520Z",
     "iopub.status.idle": "2021-05-18T15:03:58.826941Z",
     "shell.execute_reply": "2021-05-18T15:03:58.827339Z"
    },
    "papermill": {
     "duration": 1.04354,
     "end_time": "2021-05-18T15:03:58.827517",
     "exception": false,
     "start_time": "2021-05-18T15:03:57.783977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute CV = True\n",
      "train_df length = 20550\n",
      "train_df classes = 6609\n",
      "valid_df length = 6849\n",
      "valid_df classes = 2202\n",
      "test_df length = 6851\n",
      "test_df classes = 2203\n"
     ]
    }
   ],
   "source": [
    "### Create Dataloader\n",
    "\n",
    "print(\"Compute CV =\", CFG.compute_cv)\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "df['target'] = df.label_group.map(df.groupby('label_group').posting_id.agg('unique').to_dict())\n",
    "\n",
    "labelencoder= LabelEncoder()\n",
    "df['label_group'] = labelencoder.fit_transform(df['label_group'])\n",
    "\n",
    "gkf = GroupKFold(n_splits=CFG.N_SPLITS)\n",
    "df['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(gkf.split(X=df, groups=df['label_group'])):\n",
    "    df.loc[valid_idx, 'fold'] = i\n",
    "\n",
    "train_df = df[df['fold']!=CFG.TEST_FOLD].reset_index(drop=True)\n",
    "train_df = train_df[train_df['fold']!=CFG.VALID_FOLD].reset_index(drop=True)\n",
    "valid_df = df[df['fold']==CFG.VALID_FOLD].reset_index(drop=True)\n",
    "test_df = df[df['fold']==CFG.TEST_FOLD].reset_index(drop=True)\n",
    "\n",
    "# force label_group to be integers from 0 to (n_class - 1)\n",
    "train_df['label_group'] = labelencoder.fit_transform(train_df['label_group'])\n",
    "\n",
    "print(\"train_df length =\", len(train_df))\n",
    "print(\"train_df classes =\", len(train_df['label_group'].unique()))\n",
    "print(\"valid_df length =\", len(valid_df))\n",
    "print(\"valid_df classes =\", len(valid_df['label_group'].unique()))\n",
    "print(\"test_df length =\", len(test_df))\n",
    "print(\"test_df classes =\", len(test_df['label_group'].unique()))\n",
    "\n",
    "train_dataset = TitleDataset(train_df, 'title', 'label_group')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = CFG.batch_size,\n",
    "    num_workers = CFG.num_workers,\n",
    "    pin_memory = True,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "valid_dataset = TitleDataset(valid_df, 'title', 'label_group')\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = CFG.batch_size,\n",
    "    num_workers = CFG.num_workers,\n",
    "    pin_memory = True,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "test_dataset = TitleDataset(test_df, 'title', 'label_group')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = CFG.batch_size,\n",
    "    num_workers = CFG.num_workers,\n",
    "    pin_memory = True,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "junior-shark",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:03:58.868817Z",
     "iopub.status.busy": "2021-05-18T15:03:58.868205Z",
     "iopub.status.idle": "2021-05-18T15:04:09.666223Z",
     "shell.execute_reply": "2021-05-18T15:04:09.665773Z"
    },
    "papermill": {
     "duration": 10.819551,
     "end_time": "2021-05-18T15:04:09.666346",
     "exception": false,
     "start_time": "2021-05-18T15:03:58.846795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for ../input/bertmodel/distilbert-base-indonesian model\n"
     ]
    }
   ],
   "source": [
    "### Create Model\n",
    "\n",
    "model = ShopeeBertModel()\n",
    "model.to(CFG.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "endless-growth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:04:09.712117Z",
     "iopub.status.busy": "2021-05-18T15:04:09.711577Z",
     "iopub.status.idle": "2021-05-18T15:04:14.236189Z",
     "shell.execute_reply": "2021-05-18T15:04:14.235757Z"
    },
    "papermill": {
     "duration": 4.550397,
     "end_time": "2021-05-18T15:04:14.236314",
     "exception": false,
     "start_time": "2021-05-18T15:04:09.685917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_start\n",
      "------------------------------\n",
      "Parameter Group 0 : 7.5e-06\n",
      "Parameter Group 1 : 1.5e-05\n",
      "Parameter Group 2 : 1.5e-05\n",
      "Parameter Group 3 : 1.5e-05\n"
     ]
    }
   ],
   "source": [
    "### Create Optimizer\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.backbone.parameters(), 'lr': CFG.scheduler_params['lr_start']},\n",
    "    {'params': model.classifier.parameters(), 'lr': CFG.scheduler_params['lr_start'] * 2},\n",
    "    {'params': model.bn.parameters(), 'lr': CFG.scheduler_params['lr_start'] * 2},\n",
    "    {'params': model.final.parameters(), 'lr': CFG.scheduler_params['lr_start'] * 2},\n",
    "]\n",
    "\n",
    "if CFG.use_sam:\n",
    "    from transformers import AdamW\n",
    "    optimizer = AdamW\n",
    "    optimizer = SAM(optimizer_grouped_parameters, optimizer)\n",
    "\n",
    "else:\n",
    "    from transformers import AdamW\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "print(\"lr_start\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(len(optimizer.param_groups)):\n",
    "    print('Parameter Group ' + str(i) + ' :', optimizer.param_groups[i][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comprehensive-nickname",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:04:14.280338Z",
     "iopub.status.busy": "2021-05-18T15:04:14.279614Z",
     "iopub.status.idle": "2021-05-18T15:04:14.282484Z",
     "shell.execute_reply": "2021-05-18T15:04:14.282042Z"
    },
    "papermill": {
     "duration": 0.026333,
     "end_time": "2021-05-18T15:04:14.282599",
     "exception": false,
     "start_time": "2021-05-18T15:04:14.256266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create Scheduler\n",
    "\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.cosine_epo-2, eta_min=CFG.eta_min, last_epoch=-1)\n",
    "scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=CFG.multiplier, total_epoch=CFG.warmup_epo,\n",
    "                                     after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-plate",
   "metadata": {
    "papermill": {
     "duration": 0.01927,
     "end_time": "2021-05-18T15:04:14.321416",
     "exception": false,
     "start_time": "2021-05-18T15:04:14.302146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fuzzy-representative",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:04:14.364593Z",
     "iopub.status.busy": "2021-05-18T15:04:14.364027Z",
     "iopub.status.idle": "2021-05-18T15:04:14.366661Z",
     "shell.execute_reply": "2021-05-18T15:04:14.367092Z"
    },
    "papermill": {
     "duration": 0.026162,
     "end_time": "2021-05-18T15:04:14.367203",
     "exception": false,
     "start_time": "2021-05-18T15:04:14.341041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs = 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Training epochs =\", CFG.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "delayed-circle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:04:14.414233Z",
     "iopub.status.busy": "2021-05-18T15:04:14.413400Z",
     "iopub.status.idle": "2021-05-18T15:22:38.437172Z",
     "shell.execute_reply": "2021-05-18T15:22:38.438450Z"
    },
    "papermill": {
     "duration": 1104.051914,
     "end_time": "2021-05-18T15:22:38.438704",
     "exception": false,
     "start_time": "2021-05-18T15:04:14.386790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 1: 100%|█████████████| 1284/1284 [02:09<00:00,  9.93it/s, loss=25.900951, LR=7.5e-6]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.28it/s]\n",
      "Training epoch: 2:   0%|                                                   | 0/1284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.1588778281800693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 2: 100%|████████████| 1284/1284 [02:06<00:00, 10.13it/s, loss=22.148993, LR=5.38e-5]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.11it/s]\n",
      "Training epoch: 3:   0%|                                                   | 0/1284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.2047229488149976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 3: 100%|█████████████| 1284/1284 [02:08<00:00, 10.00it/s, loss=16.716818, LR=0.0001]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 45.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.43426307750091925\n",
      "[2021-05-18 15:11:01.126259] Valid f1 score improved. Saving model weights to ./distilbert-base-indonesian_epoch8-bs16x1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 4: 100%|█████████████| 1284/1284 [02:09<00:00,  9.94it/s, loss=11.437209, LR=0.0001]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.6323399630538924\n",
      "[2021-05-18 15:13:19.414025] Valid f1 score improved. Saving model weights to ./distilbert-base-indonesian_epoch8-bs16x1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 5: 100%|█████████████| 1284/1284 [02:11<00:00,  9.79it/s, loss=7.424342, LR=8.94e-5]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 45.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.7626602878654173\n",
      "[2021-05-18 15:15:39.063955] Valid f1 score improved. Saving model weights to ./distilbert-base-indonesian_epoch8-bs16x1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 6: 100%|█████████████| 1284/1284 [02:09<00:00,  9.91it/s, loss=4.435408, LR=6.37e-5]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 44.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.7888959905259589\n",
      "[2021-05-18 15:17:57.932119] Valid f1 score improved. Saving model weights to ./distilbert-base-indonesian_epoch8-bs16x1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 7: 100%|██████████████| 1284/1284 [02:10<00:00,  9.83it/s, loss=2.622678, LR=3.8e-5]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.7975856786812485\n",
      "[2021-05-18 15:20:17.211070] Valid f1 score improved. Saving model weights to ./distilbert-base-indonesian_epoch8-bs16x1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 8: 100%|█████████████| 1284/1284 [02:12<00:00,  9.72it/s, loss=1.813817, LR=2.74e-5]\n",
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid f1 score = 0.7988446667868158\n",
      "[2021-05-18 15:22:37.289471] Valid f1 score improved. Saving model weights to ./distilbert-base-indonesian_epoch8-bs16x1.pt\n"
     ]
    }
   ],
   "source": [
    "max_f1_valid = 0.\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    model, avg_loss_train = train_fn(model, train_dataloader, optimizer, scheduler,\n",
    "                                     CFG.use_sam, CFG.accum_iter, epoch, CFG.device, CFG.use_amp)\n",
    "\n",
    "    valid_embeddings = get_bert_embeddings(valid_df, 'title', model)\n",
    "    valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n",
    "                                      knn=CFG.bert_knn if len(df) > 3 else 3, threshold=CFG.bert_knn_threshold)\n",
    "\n",
    "    valid_df['oof'] = valid_predictions\n",
    "    valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n",
    "    valid_f1 = valid_df.f1.mean()\n",
    "    print('Valid f1 score =', valid_f1)\n",
    "\n",
    "    if (epoch >= CFG.min_save_epoch) and (valid_f1 > max_f1_valid):\n",
    "        print(f\"[{datetime.datetime.now()}] Valid f1 score improved. Saving model weights to {CFG.save_model_path}\")\n",
    "        max_f1_valid = valid_f1\n",
    "        torch.save(model.state_dict(), CFG.save_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-brick",
   "metadata": {
    "papermill": {
     "duration": 4.719101,
     "end_time": "2021-05-18T15:22:47.383043",
     "exception": false,
     "start_time": "2021-05-18T15:22:42.663942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Best threshold Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "duplicate-brooks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:22:56.028440Z",
     "iopub.status.busy": "2021-05-18T15:22:56.027577Z",
     "iopub.status.idle": "2021-05-18T15:24:30.379591Z",
     "shell.execute_reply": "2021-05-18T15:24:30.380463Z"
    },
    "papermill": {
     "duration": 98.554154,
     "end_time": "2021-05-18T15:24:30.380659",
     "exception": false,
     "start_time": "2021-05-18T15:22:51.826505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_bert_embeddings:   0%|                              | 0/216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.1 -> f1 score = 0.5946931640927121\n",
      "threshold = 0.11 -> f1 score = 0.6079904640388761\n",
      "threshold = 0.12 -> f1 score = 0.618516866065996\n",
      "threshold = 0.13 -> f1 score = 0.6302470542099927\n",
      "threshold = 0.14 -> f1 score = 0.6402565908954716\n",
      "threshold = 0.15 -> f1 score = 0.6499442455212128\n",
      "threshold = 0.16 -> f1 score = 0.6608208754794432\n",
      "threshold = 0.17 -> f1 score = 0.6695923740555989\n",
      "threshold = 0.18 -> f1 score = 0.6794313597927569\n",
      "threshold = 0.19 -> f1 score = 0.6890340116559348\n",
      "threshold = 0.2 -> f1 score = 0.69910388876038\n",
      "threshold = 0.21 -> f1 score = 0.707891895309961\n",
      "threshold = 0.22 -> f1 score = 0.7192357616589128\n",
      "threshold = 0.23 -> f1 score = 0.7281961803676481\n",
      "threshold = 0.24 -> f1 score = 0.7381275956859327\n",
      "threshold = 0.25 -> f1 score = 0.7456950044773262\n",
      "threshold = 0.26 -> f1 score = 0.7520239018904966\n",
      "threshold = 0.27 -> f1 score = 0.7588373628073453\n",
      "threshold = 0.28 -> f1 score = 0.7657535520569626\n",
      "threshold = 0.29 -> f1 score = 0.7731071435497937\n",
      "threshold = 0.3 -> f1 score = 0.7779660484314519\n",
      "threshold = 0.31 -> f1 score = 0.7819462305754052\n",
      "threshold = 0.32 -> f1 score = 0.7853958203862369\n",
      "threshold = 0.33 -> f1 score = 0.7884929176907444\n",
      "threshold = 0.34 -> f1 score = 0.7925892835413186\n",
      "threshold = 0.35 -> f1 score = 0.7947378630386597\n",
      "threshold = 0.36 -> f1 score = 0.7976425089338907\n",
      "threshold = 0.37 -> f1 score = 0.7984440969709637\n",
      "threshold = 0.38 -> f1 score = 0.7989680779704147\n",
      "threshold = 0.39 -> f1 score = 0.7998681693213604\n",
      "threshold = 0.4 -> f1 score = 0.7988446667868158\n",
      "threshold = 0.41 -> f1 score = 0.7960750571090168\n",
      "threshold = 0.42 -> f1 score = 0.7918483173041235\n",
      "threshold = 0.43 -> f1 score = 0.7867585865935038\n",
      "threshold = 0.44 -> f1 score = 0.7803164645475795\n",
      "threshold = 0.45 -> f1 score = 0.7719173600239\n",
      "threshold = 0.46 -> f1 score = 0.7592292700760069\n",
      "threshold = 0.47 -> f1 score = 0.7445358717737619\n",
      "threshold = 0.48 -> f1 score = 0.7276856756680647\n",
      "threshold = 0.49 -> f1 score = 0.7073429693706895\n",
      "Best threshold = 0.39\n",
      "Best f1 score = 0.7998681693213604\n"
     ]
    }
   ],
   "source": [
    "print(\"Searching best threshold...\")\n",
    "\n",
    "search_space = np.arange(10, 50, 1)\n",
    "\n",
    "model.load_state_dict(torch.load(CFG.save_model_path, map_location=CFG.device))\n",
    "valid_embeddings = get_bert_embeddings(valid_df, 'title', model)\n",
    "\n",
    "best_f1_valid = 0.\n",
    "best_threshold = 0.\n",
    "\n",
    "for i in search_space:\n",
    "    threshold = i / 100\n",
    "    valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n",
    "                                      knn=CFG.bert_knn if len(df) > 3 else 3, threshold=threshold)\n",
    "\n",
    "    valid_df['oof'] = valid_predictions\n",
    "    valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n",
    "    valid_f1 = valid_df.f1.mean()\n",
    "    print(f\"threshold = {threshold} -> f1 score = {valid_f1}\")\n",
    "\n",
    "    if (valid_f1 > best_f1_valid):\n",
    "        best_f1_valid = valid_f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(\"Best threshold =\", best_threshold)\n",
    "print(\"Best f1 score =\", best_f1_valid)\n",
    "BEST_THRESHOLD = best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "focused-recorder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:24:38.870572Z",
     "iopub.status.busy": "2021-05-18T15:24:38.869638Z",
     "iopub.status.idle": "2021-05-18T15:25:21.829814Z",
     "shell.execute_reply": "2021-05-18T15:25:21.830526Z"
    },
    "papermill": {
     "duration": 47.193861,
     "end_time": "2021-05-18T15:25:21.830727",
     "exception": false,
     "start_time": "2021-05-18T15:24:34.636866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best knn...\n",
      "knn = 40 -> f1 score = 0.7992375536007583\n",
      "knn = 42 -> f1 score = 0.7994054035086529\n",
      "knn = 44 -> f1 score = 0.7995570121607591\n",
      "knn = 46 -> f1 score = 0.7997094041952095\n",
      "knn = 48 -> f1 score = 0.7998187083962928\n",
      "knn = 50 -> f1 score = 0.7998681693213604\n",
      "knn = 52 -> f1 score = 0.7998869234622342\n",
      "knn = 54 -> f1 score = 0.7998738280840724\n",
      "knn = 56 -> f1 score = 0.7998738280840724\n",
      "knn = 58 -> f1 score = 0.7998738280840724\n",
      "knn = 60 -> f1 score = 0.7998738280840724\n",
      "knn = 62 -> f1 score = 0.7998738280840724\n",
      "knn = 64 -> f1 score = 0.7998738280840724\n",
      "knn = 66 -> f1 score = 0.7998738280840724\n",
      "knn = 68 -> f1 score = 0.7998738280840724\n",
      "knn = 70 -> f1 score = 0.7998738280840724\n",
      "knn = 72 -> f1 score = 0.7998738280840724\n",
      "knn = 74 -> f1 score = 0.7998738280840724\n",
      "knn = 76 -> f1 score = 0.7998738280840724\n",
      "knn = 78 -> f1 score = 0.7998738280840724\n",
      "Best knn = 52\n",
      "Best f1 score = 0.7998869234622342\n"
     ]
    }
   ],
   "source": [
    "print(\"Searching best knn...\")\n",
    "\n",
    "search_space = np.arange(40, 80, 2)\n",
    "\n",
    "best_f1_valid = 0.\n",
    "best_knn = 0\n",
    "\n",
    "for knn in search_space:\n",
    "\n",
    "    valid_predictions = get_neighbors(valid_df, valid_embeddings.detach().cpu().numpy(),\n",
    "                                      knn=knn, threshold=BEST_THRESHOLD)\n",
    "\n",
    "    valid_df['oof'] = valid_predictions\n",
    "    valid_df['f1'] = valid_df.apply(getMetric('oof'), axis=1)\n",
    "    valid_f1 = valid_df.f1.mean()\n",
    "    print(f\"knn = {knn} -> f1 score = {valid_f1}\")\n",
    "\n",
    "    if (valid_f1 > best_f1_valid):\n",
    "        best_f1_valid = valid_f1\n",
    "        BEST_KNN = knn\n",
    "\n",
    "print(\"Best knn =\", BEST_KNN)\n",
    "print(\"Best f1 score =\", best_f1_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-sending",
   "metadata": {
    "papermill": {
     "duration": 4.426224,
     "end_time": "2021-05-18T15:25:30.707859",
     "exception": false,
     "start_time": "2021-05-18T15:25:26.281635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Find Test F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bottom-seeking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:25:39.678487Z",
     "iopub.status.busy": "2021-05-18T15:25:39.677653Z",
     "iopub.status.idle": "2021-05-18T15:25:46.854618Z",
     "shell.execute_reply": "2021-05-18T15:25:46.855466Z"
    },
    "papermill": {
     "duration": 11.400674,
     "end_time": "2021-05-18T15:25:46.855659",
     "exception": false,
     "start_time": "2021-05-18T15:25:35.454985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_bert_embeddings: 100%|████████████████████| 216/216 [00:04<00:00, 46.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score = 0.8117166551622531\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_bert_embeddings(test_df, 'title', model)\n",
    "test_predictions = get_neighbors(test_df, test_embeddings.detach().cpu().numpy(),\n",
    "                                      knn=BEST_KNN, threshold=BEST_THRESHOLD)\n",
    "\n",
    "test_df['oof'] = test_predictions\n",
    "test_df['f1'] = test_df.apply(getMetric('oof'), axis=1)\n",
    "test_f1 = test_df.f1.mean()\n",
    "print(\"Test f1 score =\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "undefined-correlation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T15:25:55.385292Z",
     "iopub.status.busy": "2021-05-18T15:25:55.383759Z",
     "iopub.status.idle": "2021-05-18T15:25:55.387766Z",
     "shell.execute_reply": "2021-05-18T15:25:55.387146Z"
    },
    "papermill": {
     "duration": 4.260063,
     "end_time": "2021-05-18T15:25:55.387913",
     "exception": false,
     "start_time": "2021-05-18T15:25:51.127850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 22 min 1 sec\n",
      "2021-05-18 15:25:55.381585\n"
     ]
    }
   ],
   "source": [
    "time_elapsed = time.time() - start_time\n",
    "print('Elapsed time: {:.0f} min {:.0f} sec'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-anniversary",
   "metadata": {
    "papermill": {
     "duration": 4.73354,
     "end_time": "2021-05-18T15:26:04.544270",
     "exception": false,
     "start_time": "2021-05-18T15:25:59.810730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1343.515596,
   "end_time": "2021-05-18T15:26:11.100932",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-18T15:03:47.585336",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
